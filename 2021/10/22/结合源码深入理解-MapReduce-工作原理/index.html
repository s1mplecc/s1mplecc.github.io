<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"s1mplecc.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="MapReduce 架构 本文讨论的 MapReduce 架构是 Hadoop 1.0 版本时的架构，从 Hadoop 2.0 开始，Hadoop 推出了资源管理框架 YARN。在 YARN 中，使用 ResourceManager 来负责容器的调度（任务运行在容器中），以及作业的管理。使用 NodeManager 来向 ResourceManager 汇报节点资源以及容器运行状态，NodeMan">
<meta property="og:type" content="article">
<meta property="og:title" content="结合源码深入理解 MapReduce 工作原理">
<meta property="og:url" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="芥子屋">
<meta property="og:description" content="MapReduce 架构 本文讨论的 MapReduce 架构是 Hadoop 1.0 版本时的架构，从 Hadoop 2.0 开始，Hadoop 推出了资源管理框架 YARN。在 YARN 中，使用 ResourceManager 来负责容器的调度（任务运行在容器中），以及作业的管理。使用 NodeManager 来向 ResourceManager 汇报节点资源以及容器运行状态，NodeMan">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png">
<meta property="og:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/2.jpg">
<meta property="og:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/3.png">
<meta property="og:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/4.png">
<meta property="og:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/5.png">
<meta property="article:published_time" content="2021-10-22T07:36:44.000Z">
<meta property="article:modified_time" content="2021-12-17T05:47:55.745Z">
<meta property="article:author" content="s1mple">
<meta property="article:tag" content="BigData">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png">

<link rel="canonical" href="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>结合源码深入理解 MapReduce 工作原理 | 芥子屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">芥子屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-友链">

    <a href="/links/" rel="section"><i class="fa fa-link fa-fw"></i>友链</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/s1mplecc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          结合源码深入理解 MapReduce 工作原理
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-22 15:36:44" itemprop="dateCreated datePublished" datetime="2021-10-22T15:36:44+08:00">2021-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-17 13:47:55" itemprop="dateModified" datetime="2021-12-17T13:47:55+08:00">2021-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Concepts/" itemprop="url" rel="index"><span itemprop="name">Concepts</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>17k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>44 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="MapReduce-架构"><a href="#MapReduce-架构" class="headerlink" title="MapReduce 架构"></a>MapReduce 架构</h2><blockquote>
<p>本文讨论的 MapReduce 架构是 Hadoop 1.0 版本时的架构，从 Hadoop 2.0 开始，Hadoop 推出了资源管理框架 YARN。在 YARN 中，使用 ResourceManager 来负责容器的调度（任务运行在容器中），以及作业的管理。使用 NodeManager 来向 ResourceManager 汇报节点资源以及容器运行状态，NodeManager 负责创建并管理执行任务的容器。从职责方面，ResourceManager 等同于 JobTracker，NodeManager 等同于 TaskTracker。大多数的分布式框架都符合这种主从设计，如 HDFS 的 NameNode 和 DataNode、Spark 的 Driver 和 Executor 等。</p>
</blockquote>
<p>以下是 MapReduce 架构和工作流程中的常用术语：</p>
<ul>
<li>Job（作业）：是客户端需要执行的一个工作单元，包括输入数据、MapReduce 程序和配置信息；</li>
<li>Task（任务）：Hadoop 将作业分成若干个任务来执行，任务分为两类，即 Map 任务和 Reduce 任务；</li>
<li>Map/Reduce：从执行阶段来看，Map 和 Reduce 代表两个大类阶段。从计算模型角度看，它们代表两个计算步骤。从代码层面看，它们是定义在 Mapper 和 Reducer 类中的函数；</li>
<li>Mapper/Reducer：从执行阶段的详细划分来看，Mapper 和 Reducer 代表执行 map 与 reduce 函数的步骤。从代码层面看，这是定义在代码中的两个 Java 类。某些语境下，Mapper 可以指代 Map 任务，Reducer 同理。</li>
</ul>
<p>与多数的大数据分布式框架相同，MapReduce 的架构也遵循主从结构：</p>
<ul>
<li>运行在 HDFS NameNode 主节点上的 <strong>JobTracker</strong> 程序，负责接收从客户端提交的 Job，将其划分成 Map 任务和 Reduce 任务，分发给从节点 TaskTracker 执行。JobTracker 负责任务之间的协作，并通过 TaskTracker 发送来的心跳包维护集群的运行状态，以及作业进度信息。</li>
<li>多个运行在 HDFS DataNode 节点上的 <strong>TaskTracker</strong> 程序，负责执行 Map 任务和 Reduce 任务，直接与 HDFS 交互。每隔一段时间，TaskTracker 向 JobTracker 发送心跳包，汇报节点运行状态，以及任务完成进度。</li>
</ul>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png" alt="66b01a299a89595ca02de7b5b9f02b85.png"></p>
<p>为了减少网络传输带来的性能影响，JobTracker 在分发 Map 任务时基于<strong>数据本地化优化</strong>（Data locality optimization）策略，将 Map 任务分发给包含此 Map 处理数据的从节点，并将程序 Jar 包发送给该 TaskTracker，遵循“运算移动，数据不移动”的原则。</p>
<h2 id="新旧版本-API-说明"><a href="#新旧版本-API-说明" class="headerlink" title="新旧版本 API 说明"></a>新旧版本 API 说明</h2><p>本文中的源码是 Hadoop 3.2.0 版本的源码，使用的是 Hadoop 新版 API。新版 API 位于 <code>org.apache.hadoop.mapreduce</code> 包下，旧版 API 位于 <code>org.apache.hadoop.mapred</code> 包下，两版 API 并不兼容，你可以在 <a target="_blank" rel="noopener" href="https://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api">这里</a> 查看两者的区别。为保证向后兼容，Hadoop 并没有移除旧版 API，因此依赖库中两个版本并存，使用时要注意。</p>
<p>设计新版 API 的主要目的是给用户提供更加简洁优雅的接口，框架的核心代码并没有调整包，比如执行 Map 与 Reduce 任务的 MapTask 和 ReduceTask 类仍位于 <code>org.apache.hadoop.mapred</code> 包下。但由于使用了两套 API，在该类中你会经常看到以 Old/New 命名的类或方法，如 <code>runNewMapper()</code> 和 <code>runOldMapper()</code>。</p>
<p>在旧版本中，Mapper 与 Reducer 被定义为接口，而在新版本中被定义为具体类，并提供默认的 map 和 reduce 实现：仅是通过 <code>context.write()</code> 重新写回数据。旧版本的 Mapper 接口源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Mapper</span>&lt;<span class="title">K1</span>, <span class="title">V1</span>, <span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">map</span><span class="params">(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>新版本中广泛使用上下文 Context 对象，整合了 OutputCollector、Reporter  和部分 JobConf 的功能，来实现与 MapReduce 系统的通信，如使用 <code>context.write()</code> 代替旧版的 <code>output.collect()</code> 功能。</p>
<p>新版本使用 Job 类替换旧版的 JobClient 类，实现作业控制，并使用统一的配置类 Configuration 来替换旧版的 JobConf 类，用于作业配置。因此，现在的一个 MapReduce 程序的入口函数可能如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    </span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    </span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    </span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    </span><br><span class="line">    System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce-中的数据格式"><a href="#MapReduce-中的数据格式" class="headerlink" title="MapReduce 中的数据格式"></a>MapReduce 中的数据格式</h2><p>MapReduce 工作过程中每个阶段的输入和输出数据都是以<strong>键值对</strong>的形式出现，如下表所示，表中的 k 与 v 代表了数据类型，不同下标代表不同的数据类型。</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">输入</th>
<th align="center">输出</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>map()</code></td>
<td align="center">&lt;$k_1$, $v_1$&gt;</td>
<td align="center">[&lt;$k_2$, $v_2$&gt;, …]</td>
<td align="center">一个输入分片被 Map 处理成一系列的键值对</td>
</tr>
<tr>
<td align="center"><code>reduce()</code></td>
<td align="center">&lt;$k_2$, [$v_2$, …]&gt;</td>
<td align="center">&lt;$k_3$, $v_3$&gt;</td>
<td align="center">Reduce 的输入键类型为 Map 后的输出键类型，输入值是键对应的值的集合</td>
</tr>
</tbody></table>
<p>下面以统计词频程序 WordCount 为例，来说明 MapReduce 过程中的数据类型转换。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>在 Mapper 工作之前，框架从文件系统中读取文件并切分为分片，将数据转换为 <code>&lt;KEYIN, VALUEIN&gt;</code> 格式的键值对传给 map 函数。在 WordCount 程序中可以理解为形如 <code>&lt;行号, &quot;a b c&quot;&gt;</code> 的输入数据。实际上，输入键“行号”在实际代码中可能如下的 LongWritable 类型，一个可序列化的长整型偏移量（offset）。这是由 Hadoop 框架定义的数据类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            Text outputKey = <span class="keyword">new</span> Text(word.toUpperCase().trim());</span><br><span class="line">            IntWritable outputValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">            context.write(outputKey, outputValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：Hadoop 提供了一套可优化网络序列化传输的基本类型，而不直接使用 Java 内置类型，这些类型位于 <code>org.apache.hadoop.io</code> 包中。上述代码中的 LongWritable 相当于 Java 的 Long 类型，Text 相当于 String 类型，IntWritable 相当于 Integer 类型。</p>
<p>map 函数将输入的文本串切分为一个个单词，输入的键值对被转换为 <code>&lt;KEYOUT, VALUEOUT&gt;</code> 格式的<strong>中间键值对</strong>输出。此时的输出键为单词，输出值为单词计数，默认为 1。即经由 Mapper 处理后，原本的“一行”数据被转换为了形如 <code>[&lt;&quot;a&quot;, 1&gt;, &lt;&quot;b&quot;, 1&gt;, &lt;&quot;c&quot;, 1&gt;]</code> 的中间数据。</p>
<p>中间数据是暂时数据，不会存入 HDFS，但是会存入运行 Map 任务节点的本地磁盘，经过数据混洗后被 Reducer 端消费。</p>
<h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><p>由于 Reduce 任务与 Map 任务与不一定处于同一节点上，Reduce 任务会通过网络通信拉取多个 Map 任务产生的中间数据。数据从 Map 任务端传输给 Reduce 任务端的过程被称为<strong>数据混洗</strong>。</p>
<p>混洗之后传入给 Reducer 的输入键值对的值，是该键对应的值的集合（可迭代对象），如 <code>&lt;&quot;a&quot;, [1, 2, 1]&gt;</code>。值可能为 2 是因为，为了减少磁盘写入和网络传输的数据量，Map 任务可能会在本地节点上预先聚合，这也就是 Combiner 所做的工作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text word, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(word, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>经过 reduce 函数处理后，单词的所有计数值被累加，输出形如 <code>&lt;&quot;a&quot;, 4&gt;</code> 的键值对。</p>
<h2 id="MapReduce-工作流程详解"><a href="#MapReduce-工作流程详解" class="headerlink" title="MapReduce 工作流程详解"></a>MapReduce 工作流程详解</h2><p>MapReduce 将处理过程分成两个大类阶段：Map 阶段和 Reduce 阶段。在 Map 阶段，由于任务分发基于数据本地化原则，Map 任务运行在包含有该任务处理数据的节点上，数据存储在当前节点的 HDFS DataNode 中。因此，Map 阶段处理的都是本地数据，不需要进行网络传输。Map 阶段产生的中间数据将会暂存在当前节点上，Reduce 阶段需要从相关节点上拉取数据进行聚合运算，再将结果写入 HDFS。因此，Reduce 阶段既需要磁盘读写，也需要网络传输。</p>
<p>当然，MapReduce 在工作时还要细分为许多小阶段，下面这张图很好的展示了 MapReduce 的整个工作流程，具体包括：数据读入阶段、Mapper 处理阶段、优化性阶段 Combiner、中间数据分区 Partitioner 阶段、被称为 MapReduce “心脏”的数据混洗 Shuffle 阶段、Reducer 处理阶段以及数据写入阶段。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/2.jpg" alt="9b17e681d1c5745840c517209dd2808f.jpeg"></p>
<p>从大类阶段上划分，数据混洗之前的阶段都可以划分到 Map 阶段，因为这些步骤都是在本地节点上完成的，不涉及网络传输。而在数据混洗阶段，程序从本地或是集群上的其他节点拉取并拷贝 Mapper 产生的中间数据，提供给 Reducer 作为输入。因此，可以将包括数据混洗及其之后的步骤划分为 Reduce 阶段。事实上，数据混洗的相关类也被定义在 <code>org.apache.hadoop.mapreduce.task.reduce</code> 包下。</p>
<p>下面将结合源码详细介绍各个阶段的工作。</p>
<h3 id="数据读入阶段"><a href="#数据读入阶段" class="headerlink" title="数据读入阶段"></a>数据读入阶段</h3><p>数据被 Mapper 处理前，需要先转换为 Mapper 支持的键值对类型，这个过程由 InputFormat 和 RecordReader 类完成。首先，由 InputFormat 类从 HDFS 读入文件并创建<strong>输入分片</strong>（Input Split），分片为等长的逻辑数据块，比如可能是形如 <code>(input-file-path, start, offset)</code> 的元组。</p>
<p>一个合理的分片大小应该与 HDFS 块大小保持一致，默认为 128 MB。<strong>Hadoop 会在分片数据所在的物理节点寻找一个空闲的 Map 槽，运行 Map 任务</strong>，由该任务运行用户自定义的 map 函数从而处理分片中的每条记录。此时，符合之前所说的数据本地化原则。一旦一个分片跨越两个物理块，由于 HDFS 的分布式存储特性，这两个块极可能位于不同的 DataNode 上，此时分片中的部分数据就需要通过网络传输到 Map 任务运行的节点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">InputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> RecordReader&lt;K, V&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>InputFormat 会为一个分片创建一个 RecordReader 对象，负责将输入分片转换为 Mapper 可处理的键值对。从数据视图角度看，输入数据字节流被转换为了面向记录的视图。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordReader</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>&gt; <span class="keyword">implements</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KEYIN <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> VALUEIN <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Mapper-阶段"><a href="#Mapper-阶段" class="headerlink" title="Mapper 阶段"></a>Mapper 阶段</h3><p>在这个阶段，Mapper 会执行用户定义的 map 函数，并将输入的键值对转换为中间键值对序列。在 MapTask 类中，使用新版本的 Mapper 会直接调用 <code>mapper.run()</code> 方法运行，旧版本还需要由包装有 Mapper 的 MapRunner 来运行。新版的 API 中，允许用户覆盖 <code>run()</code> 方法，以及 Context 对象的生命周期方法，来做到对 Mapper 执行的完全控制。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mapper</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(KEYIN key, VALUEIN value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        setup(context);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">                map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：Map 阶段的整个工作流程，可以在 MapTask 类的 <code>run()</code> 和 <code>runNewMapper()</code> 或 <code>runOldMapper()</code> 方法中清晰的一览。</p>
<h3 id="Partitioner-阶段"><a href="#Partitioner-阶段" class="headerlink" title="Partitioner 阶段"></a>Partitioner 阶段</h3><p>与 Map 任务不同，Reduce 任务并不具备数据本地化的优势，单个 Reduce 任务的输入通常来自于所有 Mapper 的输出。因此，即使 Reduce 任务与某些个 Map 任务处于同一节点上，也不可避免的需要通过网络传输从其他节点上获取 Mapper 输出。</p>
<p>当只有一个 Reduce 任务时，这个 Reduce 任务会读取所有 Mapper 输出，此时对中间数据分区意义不大，因为所有 Mapper 输出都被写入同一文件。但当有多个 Reduce 任务时，为提高数据吞吐量，<strong>每个 Map 任务会针对输出进行分区（Partition），即为每个 Reduce 任务创建一个分区</strong>。同一个键对应的键值对记录都被划分在同一分区中，每个分区中可以包含许多键（及其对应的值）。并且，一个分区中的记录是<strong>按键排序</strong>的，这样，磁盘读取一个键的所有记录时能保证读取连续数据，而不是从零散的文件中再过滤数据。</p>
<h4 id="数据溢写"><a href="#数据溢写" class="headerlink" title="数据溢写"></a>数据溢写</h4><p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/3.png" alt="20cbf28f315007e05a1c762915a25ca2.png"></p>
<p>每个 Map 任务有一个环形内存缓冲区，缓冲区大小由 <code>io.sort.mb</code> 指定，默认 100MB。Mapper 产生的中间键值对记录将被先写入缓冲区，当达到缓冲区设定阈值时（<code>io.sort.spill.percent</code>，默认 80%），会开启一个线程将内容<strong>溢写</strong>（spill）到磁盘，由 <code>mapred.local.dir</code> 属性指定的目录。线程工作的同时，Mapper 的输出继续被写到缓冲区，如果在此期间缓冲区被写满，Mapper 将会阻塞直到溢写过程结束。</p>
<p><strong>每次缓冲区达到溢出阈值，就会新建一个溢出文件，键值对会在内存中先按键排序然后写入文件</strong>。在任务完成前，溢出文件会不断合并并保证文件中数据是有序的。</p>
<p>数据溢写会写入运行 Map 任务节点的本地磁盘，并不会写入 HDFS（但 Reducer 的输出并不是这样）。这是因为，Map 任务运行的输入分片来自本地节点，输出也应写入本地节点，这样保证了整个 Map 任务的数据本地化。而写入 HDFS 则会为数据创建分布式副本，带来额外网络开销。更何况，Map 任务产生的输出只是暂时数据，任务执行完毕后会被删除。</p>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><p>下面我们通过阅读 <code>org.apache.hadoop.mapred.MapTask</code> 类的相关源码，来加深对 Map 任务将中间键值对分区、排序存入磁盘过程的理解。Hadoop 使用 OutputCollector 将 Mapper 数据写入磁盘，由于继承/实现的类不同，MapTask 类中存在 Old/NewOutputCollector 两套新旧收集器。以新版 NewOutputCollector 为例，它的实现逻辑如下：</p>
<ul>
<li>首先，创建一个排序收集器 collector，具体的排序逻辑在 MapOutputBuffer 类中，其实现了 IndexedSortable 接口的 <code>compare()</code> 方法，按键进行排序；</li>
<li>从作业上下文（配置）中获取 Reduce 任务总数；</li>
<li>当 Reduce 任务数大于一时，通过反射创建 Partitioner 类的实例，这个类可以是用户自定义的类，并在配置阶段注入；</li>
<li>如果 Reduce 任务数等于零（纯 Map 作业）或等于一，创建匿名内部类，返回的分区号为固定值，所有输出写到同一分区；</li>
<li>写入阶段通过收集器的 <code>collect()</code> 方法，将键值对按照分区号写入对应分区。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">NewOutputCollector</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> MapOutputCollector&lt;K,V&gt; collector;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Partitioner&lt;K,V&gt; partitioner;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partitions;</span><br><span class="line"></span><br><span class="line">    NewOutputCollector(JobContext jobContext, JobConf job, TaskUmbilicalProtocol umbilical, TaskReporter reporter) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">        collector = createSortingCollector(job, reporter);</span><br><span class="line">        partitions = jobContext.getNumReduceTasks();</span><br><span class="line">        <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">            partitioner = (Partitioner&lt;K,V&gt;) ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partitioner = <span class="keyword">new</span> Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        collector.collect(key, value, partitioner.getPartition(key, value, partitions));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Partitioner 类位于 <code>org.apache.hadoop.mapreduce</code> 包下，其作用是返回一个整型分区号，Map 任务将这个分区号作为写入哪个分区的标识。用户可以自定义分区函数，需要继承 Partitioner 类。通常，默认的分区函数 HashPartitioner 足够用了，它使用哈希函数，将键进行哈希后取非负（前置位补零）然后对 Reduce 任务总数取模。这样，能保证键相同的记录被分配至同一分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HashPartitioner 足够高效，但如果你执行的 MapReduce 作业发生数据倾斜的问题（如存在大部分相同键），可以考虑自定义分区函数，比如加入随机值。</p>
<h4 id="Reduce-任务数设置"><a href="#Reduce-任务数设置" class="headerlink" title="Reduce 任务数设置"></a>Reduce 任务数设置</h4><p>Hadoop MapReduce 的<strong>并行度</strong>取决于 Map 任务数量和 Reduce 任务数量。Map 任务数量不需要手动设置，原因是该数量等于输入文件被划分成的分片数，框架会为一个输入分片分配一个 Map 任务。Map 任务数据取决于输入文件的大小以及 HDFS 文件块的大小。默认情况下，输入分片大小与块大小保持一致，均为 128 MB。</p>
<p>Reduce 任务数量可以在作业配置时通过 <code>job.setNumReduceTasks()</code> 手动设置。默认情况下，只有一个 Reduce 任务，这对于本地小规模数据已经足够了。但在真实应用中，几乎所有作业都将它设置为一个较大的数字，否则，所有的中间数据都被传输给一个 Reduce 任务，作业处理极其低效。</p>
<p>为一个作业设置多少个 Reduce 任务数，与其说是一门技术，不如说更多是一门艺术。由于并行化程度提高，增加 Reducer 能缩短 Reduce 阶段整体耗时。并且，增加的 Reducer 对于解决数据倾斜问题通常能有很好的帮助。然而，如果配置了过多 Reducer，文件将被切分为更多小文件，磁盘 I/O 次数将显著增加，这又不够优化。相对于大批量的小文件，Hadoop 更适合处理少量的大文件。一条经验法则是，目标 Reducer 保持在每个运行 5 分钟左右，且产生至少一个 HDFS 块的输出比较合适。</p>
<h3 id="Combiner-阶段"><a href="#Combiner-阶段" class="headerlink" title="Combiner 阶段"></a>Combiner 阶段</h3><p>为了减少 Map 和 Reduce 任务之间的数据传输量，Hadoop 允许用户针对 Map 任务指定一个 Combiner 类。以统计词频程序为例，Mapper 输出的键值对 <code>&lt;单词，计数值&gt;</code> 的计数值初始均为一，此时可通过 Combiner 将相同键的值进行聚合，即计数值累加。这样，同一个 Mapper （来自同一输入分片）的处理数据将被提前聚合，既减少了磁盘写入数据量，也减少了需要通过网络传输给 Reducer 的数据量。因此 Combiner 也被称为 “Mini-reducer” 或 “Local-reducer”，意指在本地节点完成的。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/4.png" alt="e58767ee9dfc6d8bf818f639a0508b7f.png"></p>
<p>Combiner 工作在文件溢写的前后，具体是在 MapTask 的内部类的 <code>sortAndSpill() </code> 和 <code>mergeParts()</code> 方法中，由 <code>combinerRunner.combine()</code> 运行。在 Mapper 输出的键值对被溢写到磁盘之前，会在内存中按键排序，如果定义了 Combiner，它将在排序后的输出上运行，经过 Combiner 紧凑后的数据再写入磁盘。在溢出文件合并阶段，如果至少存在 3 个溢出文件（由 <code>mapreduce.map.combine.minspills</code> 属性指定），那么 Combiner 将会在文件合并时再次运行。因此，Combiner 可能会在 Mapper 输出上反复运行。如果只有 1 或 2 个溢出文件，此时说明 Mapper 输出吞吐量降低，因而不值得调用 Combiner 带来额外开销。</p>
<p>由于 Combiner 与 Reducer 聚合逻辑相同，Hadoop 没有提供额外的 Combiner 类，而是通过 Reducer 类复用。新版的 CombinerRunner 代码如下，它通过反射获取了一个 Reducer 实例，直接运行 Reducer 的 <code>run()</code> 方法进行数据聚合，聚合结果由上下文对象传入 OutputCollector 中，最后写入磁盘。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NewCombinerRunner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">CombinerRunner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(RawKeyValueIterator iterator, OutputCollector&lt;K, V&gt; collector)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        <span class="comment">// make a reducer</span></span><br><span class="line">        Reducer&lt;K, V, K, V&gt; reducer = (Reducer&lt;K, V, K, V&gt;) ReflectionUtils.newInstance(reducerClass, job);</span><br><span class="line">        Context reducerContext = createReduceContext(reducer, job, taskId,</span><br><span class="line">                iterator, <span class="keyword">null</span>, inputCounter,</span><br><span class="line">                <span class="keyword">new</span> OutputConverter(collector),</span><br><span class="line">                committer,</span><br><span class="line">                reporter, comparator, keyClass,</span><br><span class="line">                valueClass);</span><br><span class="line">        reducer.run(reducerContext);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用户在声明作业配置时可以直接复用定义好的 Reducer 类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">job.setReducerClass(IntSumReducer.class);</span><br></pre></td></tr></table></figure>

<h3 id="Shuffler-阶段"><a href="#Shuffler-阶段" class="headerlink" title="Shuffler 阶段"></a>Shuffler 阶段</h3><p>数据混洗（Shuffle）被称为是 MapReduce 的“心脏”，是奇迹发生的地方。从宏观上，它体现为从 Map 任务端到 Reduce 任务端的数据流处理过程，具体说即是 <strong>Mapper 产生的中间键值对被重新组织（排序、分区），写入到本地磁盘中（溢写），Reducer 从多个 Mapper 工作节点上拷贝、合并数据</strong>的过程。最终目的是将数据从 Mapper 端发送到 Reducer 端，中间过程则是 MapReduce 框架基于性能考量的优化过程。从下图你可以清晰的体会到为什么这个过程被称为“数据混洗”。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/5.png" alt="2abd2dc93aa589f4b1d42b451b2b873c.png"></p>
<p>在微观上，Shuffle 被划分为了 Reduce 任务阶段的工作，定义在 <code>org.apache.hadoop.mapreduce.task.reduce</code> 包内。它是一个具体类，是框架定义的 <code>ShuffleConsumerPlugin&lt;K, V&gt;</code> 接口的内置实现类。从接口命名上可以体现出它作为 Mapper 端输出数据“消费者”的身份，“插件”则体现了它的可插拔特性，框架允许用户使用除了内置实现以外的三方插件。</p>
<p>在 ReduceTask 类的 <code>run()</code> 方法中通过反射获取了一个 Shuffle 类实例。<code>job.getClass()</code> 的第一个参数是三方插件的配置地址，第二个值 <code>Shuffle.class</code> 即为默认的内置实现类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceTask</span> <span class="keyword">extends</span> <span class="title">Task</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(JobConf job, <span class="keyword">final</span> TaskUmbilicalProtocol umbilical)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        ShuffleConsumerPlugin shuffleConsumerPlugin = <span class="keyword">null</span>;</span><br><span class="line">        Class&lt;? extends ShuffleConsumerPlugin&gt; clazz = job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);</span><br><span class="line">        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);</span><br><span class="line"></span><br><span class="line">        shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line">        rIter = shuffleConsumerPlugin.run();</span><br><span class="line">        <span class="keyword">if</span> (useNewApi) &#123;</span><br><span class="line">            runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            runOldReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</span><br><span class="line">        &#125;</span><br><span class="line">        shuffleConsumerPlugin.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Shuffle 类的 <code>run()</code> 方法，是数据混洗步骤真正的具体实现。</p>
<h4 id="源码分析-1"><a href="#源码分析-1" class="headerlink" title="源码分析"></a>源码分析</h4><p>在一个 Map 任务完成后，会通过心跳包通知 JobTracker，这样，JobTracker 就能获取 Map 输出与主机位置之间的映射关系，Reduce 任务中的一个线程定期询问 JobTracker 以获取 Map 输出位置，直到获取所有输出位置。当一个 Map 任务完成后，Reduce 任务就可以开始复制了，这就是 <strong>Reduce 任务的复制（copy）阶段</strong>。此过程由并发线程完成，默认是 5 个线程，由 <code>mapreduce.reduce.shuffle.parallelcopies</code> 属性指定。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Shuffle</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">ShuffleConsumerPlugin</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;, <span class="title">ExceptionReporter</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RawKeyValueIterator <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// Start the map-completion events fetcher thread</span></span><br><span class="line">        <span class="keyword">final</span> EventFetcher&lt;K, V&gt; eventFetcher = <span class="keyword">new</span> EventFetcher&lt;K, V&gt;(reduceId, umbilical, scheduler, <span class="keyword">this</span>, maxEventsToFetch);</span><br><span class="line">        eventFetcher.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Start the map-output fetcher threads</span></span><br><span class="line">        <span class="keyword">boolean</span> isLocal = localMapFiles != <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> numFetchers = isLocal ? <span class="number">1</span> : jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, <span class="number">5</span>);</span><br><span class="line">        Fetcher&lt;K, V&gt;[] fetchers = <span class="keyword">new</span> Fetcher[numFetchers];</span><br><span class="line">        <span class="keyword">if</span> (isLocal) &#123;</span><br><span class="line">            fetchers[<span class="number">0</span>] = <span class="keyword">new</span> LocalFetcher&lt;K, V&gt;(jobConf, reduceId, scheduler, merger, reporter, metrics, <span class="keyword">this</span>, reduceTask.getShuffleSecret(), localMapFiles);</span><br><span class="line">            fetchers[<span class="number">0</span>].start();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numFetchers; ++i) &#123;</span><br><span class="line">                fetchers[i] = <span class="keyword">new</span> Fetcher&lt;K, V&gt;(jobConf, reduceId, scheduler, merger, reporter, metrics, <span class="keyword">this</span>, reduceTask.getShuffleSecret());</span><br><span class="line">                fetchers[i].start();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Shuffle 使用名为 Fetcher 的线程类进行数据复制，如果 Reduce 任务恰好就处于运行 Map 任务的节点上，此时不需要网络通信，直接通过 LocalFetcher 获取本地数据。否则，使用 Fetcher 通过 HTTP 通信拉取相关节点上的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fetcher</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If merge is on, block</span></span><br><span class="line">        merger.waitForResource();</span><br><span class="line">        <span class="comment">// Get a host to shuffle from</span></span><br><span class="line">        host = scheduler.getHost();</span><br><span class="line">        metrics.threadBusy();</span><br><span class="line">        <span class="comment">// Shuffle</span></span><br><span class="line">        copyFromHost(host);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注：拉取后的数据是写入内存还是写入磁盘，是由 MapOutput 类的 <code>shuffle()</code> 方法完成的，该类拥有两个具体子类：InMemoryMapOutput 和 OnDiskMapOutput。</p>
<p>如果 Map 任务输出相当小，会被复制到 Reduce 任务的 JVM 内存缓冲区中，一旦内存缓冲区达到阈值，Map 输出会被复制到磁盘。随着磁盘上副本增多，后台线程会将它们合并为更大的、按键排好序的文件，这就是 Reduce 任务的<strong>合并阶段</strong>（merge）。这个阶段将合并 Map 输出，并维持其按键排序。合并的数据可能来自于内存缓冲区和磁盘文件。</p>
<p>合并阶段按照预先设定的合并因子（默认为 10），每趟合并 10 个文件，所以合并过程是循环进行的。为减少磁盘读写次数，最后一轮的文件合并（包含所有数据）不再写入磁盘，而是直接传送给 Reducer 处理。</p>
<h3 id="Reducer-阶段"><a href="#Reducer-阶段" class="headerlink" title="Reducer 阶段"></a>Reducer 阶段</h3><p>在 Reducer 阶段，对已排序的输出中的每个键调用 reduce 函数。此阶段的输出直接写到输出文件系统，一般为 HDFS，还可以是数据库。如果采用 HDFS，由于 Reduce 任务运行所在的节点也运行 DataNode，所以第一个块副本将被写到本地磁盘中。</p>
<p>用户需要继承 Reducer 并重写 reduce 函数，函数的输入值入参是值的可迭代对象，框架会从文件系统中读取输入键对应的值集合，传入给 reduce 函数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Reducer</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (VALUEIN value : values) &#123;</span><br><span class="line">            context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>与 Mapper 类似，框架也允许用户覆盖 <code>run()</code> 方法，以及 Context 对象的生命周期方法。</p>
<h3 id="数据写入阶段"><a href="#数据写入阶段" class="headerlink" title="数据写入阶段"></a>数据写入阶段</h3><p>数据写入阶段，由 OutputFormat 负责检验作业的输出规范，如输出目录是否已存在。如果写入到文件系统，则由实现子类 FileOutputformat 的 <code>setOutputPath()</code> 方法负责从作业配置中读取输出目录。由 OutputFormat 创建的 RecordWriter 对象负责将 Reducer 产生的键值对数据写到输出文件，每个 Reducer 对应一个文件。此过程无需创建分片。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OutputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> RecordWriter&lt;K, V&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">checkOutputSpecs</span><span class="params">(JobContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> OutputCommitter <span class="title">getOutputCommitter</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce-的局限"><a href="#MapReduce-的局限" class="headerlink" title="MapReduce 的局限"></a>MapReduce 的局限</h2><p>在 MapReduce 的工作过程中，框架需要频繁地读写文件系统，MapReduce 作业往往又都是数据密集型的，因此大量的中间数据会被往复地写入、读取、合并排序后又写入磁盘。大量的磁盘 I/O 导致 MapReduce 的耗时往往是分钟级、甚至是小时级的。受到 MapReduce 诞生年代的约束，昂贵的内存迫使用户将 Hadoop 集群部署在廉价的商用机器集群上，使用磁盘来进行数据的缓存。大规模的分布式部署使得 MapReduce 程序具有高容错性和良好的横向扩展性优势。</p>
<p>时至今日，用户对数据处理延迟的忍耐性越来越低，在大规模数据批处理之上又诞生了实时性要求极高的流处理系统。不管是 Spark Streaming 还是 Flink 都能做到秒级甚至是毫秒级的响应，MapReduce 因为其性能局限已经跟不上时代的需求。</p>
<p>MapReduce 的函数本身是无状态的，这意味着并不是所有工作 MapReduce 都能胜任，比如需要状态共享和参数依赖的机器学习模型训练算法。尽管 MapReduce 可以通过文件存储状态，但这样带来的性能开销是巨大的。相反，Spark 作为一个基于内存迭代式的大数据计算引擎很适合这样的场景，其提供了有状态的流来应对需要状态共享的作业，通过 <code>updateStateByKey()</code> 和 <code>mapWithState()</code> 状态管理函数共享状态。状态被保存在内存中，后续访问直接从内存中读取。据 Spark 官方统计的 Spark 运行逻辑回归机器学习算法的运行时间要优于 hadoop 一百倍。这也让 Spark 提供机器学习库 Spark MLlib 成为可能。</p>
<p>MapReduce 框架要求用户编写底层的 map 和 reduce 函数，这对于数据分析师是一个考验，Spark SQL 允许数据分析师使用 SQL 语言处理数据，由框架负责将其翻译成底层执行步骤。而且，编写底层函数这样的控制粒度不够灵活，对于一个复杂的作业来说，可能要由多个 MapReduce 作业组合而成，这样又会引入额外的中间数据读写开销。相比之下，Spark 编程模型更灵活，提供了丰富的 Transformation 和 Action 算子。性能方面，Spark 会为作业构建逻辑执行计划图（DAG），并针对其步骤进行优化，减少数据混洗阶段的 I/O 次数。</p>
<p>不管怎么说，MapReduce 作为第一代分布式计算引擎，其后诞生的分布式框架都是在其基础上的演进，因此有必要对 MapReduce 工作原理做个详细了解。Spark 由批处理起家，延续了 MapReduce 编程模型的设计思路。下一节我们将介绍 Spark 的运行原理。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>Hadoop 权威指南（第四版）</li>
<li><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Mapper">Apache Hadoop 官方文档：MapReduce Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://techvidvan.com/tutorials/how-mapreduce-works/">Phases of MapReduce – How Hadoop MapReduce Works</a></li>
<li><a target="_blank" rel="noopener" href="http://hadoopbeforestarting.blogspot.com/2012/12/difference-between-hadoop-old-api-and.html">Difference  between Hadoop OLD API and NEW API</a></li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpeg" alt="s1mple 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>s1mple
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/" title="结合源码深入理解 MapReduce 工作原理">https://s1mplecc.github.io/2021/10/22/结合源码深入理解-MapReduce-工作原理/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/BigData/" rel="tag"><i class="fa fa-tag"></i> BigData</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/" rel="prev" title="使用 Docker 快速部署 Spark + Hadoop 大数据集群">
      <i class="fa fa-chevron-left"></i> 使用 Docker 快速部署 Spark + Hadoop 大数据集群
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/" rel="next" title="基于 Elasticsearch 与 Kibana 搭建流量可视化平台">
      基于 Elasticsearch 与 Kibana 搭建流量可视化平台 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">MapReduce 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B0%E6%97%A7%E7%89%88%E6%9C%AC-API-%E8%AF%B4%E6%98%8E"><span class="nav-number">2.</span> <span class="nav-text">新旧版本 API 说明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="nav-number">3.</span> <span class="nav-text">MapReduce 中的数据格式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mapper"><span class="nav-number">3.1.</span> <span class="nav-text">Mapper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reducer"><span class="nav-number">3.2.</span> <span class="nav-text">Reducer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.</span> <span class="nav-text">MapReduce 工作流程详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%85%A5%E9%98%B6%E6%AE%B5"><span class="nav-number">4.1.</span> <span class="nav-text">数据读入阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mapper-%E9%98%B6%E6%AE%B5"><span class="nav-number">4.2.</span> <span class="nav-text">Mapper 阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Partitioner-%E9%98%B6%E6%AE%B5"><span class="nav-number">4.3.</span> <span class="nav-text">Partitioner 阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%BA%A2%E5%86%99"><span class="nav-number">4.3.1.</span> <span class="nav-text">数据溢写</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-number">4.3.2.</span> <span class="nav-text">源码分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reduce-%E4%BB%BB%E5%8A%A1%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">4.3.3.</span> <span class="nav-text">Reduce 任务数设置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Combiner-%E9%98%B6%E6%AE%B5"><span class="nav-number">4.4.</span> <span class="nav-text">Combiner 阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffler-%E9%98%B6%E6%AE%B5"><span class="nav-number">4.5.</span> <span class="nav-text">Shuffler 阶段</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1"><span class="nav-number">4.5.1.</span> <span class="nav-text">源码分析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reducer-%E9%98%B6%E6%AE%B5"><span class="nav-number">4.6.</span> <span class="nav-text">Reducer 阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E9%98%B6%E6%AE%B5"><span class="nav-number">4.7.</span> <span class="nav-text">数据写入阶段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MapReduce-%E7%9A%84%E5%B1%80%E9%99%90"><span class="nav-number">5.</span> <span class="nav-text">MapReduce 的局限</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="s1mple"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">s1mple</p>
  <div class="site-description" itemprop="description">春光恰与少年同，十里清风慕天青</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">76</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/s1mplecc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;s1mplecc" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:s1mple951205@gmail.com" title="E-Mail → mailto:s1mple951205@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/zhang-zhen-2-52" title="知乎 → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;zhang-zhen-2-52" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://music.163.com/#/user/home?id=64441901" title="网易云 → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;64441901" rel="noopener" target="_blank"><i class="fas fa-music fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">s1mple</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">421k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">17:34</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 33231,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'LL5WuMzDNh0Y7j17go8YWYQ8-gzGzoHsz',
      appKey     : 'eGkSV4kghSvWKeQvrDPjf6ig',
      placeholder: "说点什么吧。",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
