<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"s1mplecc.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="春光恰与少年同，十里清风慕天青">
<meta property="og:type" content="website">
<meta property="og:title" content="芥子屋">
<meta property="og:url" content="https://s1mplecc.github.io/index.html">
<meta property="og:site_name" content="芥子屋">
<meta property="og:description" content="春光恰与少年同，十里清风慕天青">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="s1mple">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://s1mplecc.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>芥子屋</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">芥子屋</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/s1mplecc" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/12/17/%E7%BD%91%E7%AB%99%E5%A4%96%E9%93%BE%E7%88%AC%E8%99%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/17/%E7%BD%91%E7%AB%99%E5%A4%96%E9%93%BE%E7%88%AC%E8%99%AB/" class="post-title-link" itemprop="url">网站外链爬虫</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-17 09:45:31 / 修改时间：13:47:55" itemprop="dateCreated datePublished" datetime="2021-12-17T09:45:31+08:00">2021-12-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Projects/" itemprop="url" rel="index"><span itemprop="name">Projects</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a>项目介绍</h2><p>网站外链爬虫，使用轻量级 Web 应用框架 Flask，以 Restful 接口提供服务，支持容器化部署。项目地址：<a target="_blank" rel="noopener" href="https://github.com/s1mplecc/external-link-crawer">https://github.com/s1mplecc/external-link-crawer</a> ，需要 Python 版本 3.x。</p>
<p>对于传入的网站 URL，获取其 HTML 网页，提取外部链接的域名。包含如下几类外链：</p>
<ul>
<li>超链接，标签 <code>a</code> 下的 <code>href</code> 属性；</li>
<li>图片，标签 <code>img</code> 下的 <code>src</code> 属性；</li>
<li>外部样式文件，标签 <code>link</code> 下  <code>href</code> 属性；</li>
<li>外部 JavaScript 脚本文件，标签 <code>script</code> 下  <code>src</code> 属性。</li>
</ul>
<h2 id="本地运行"><a href="#本地运行" class="headerlink" title="本地运行"></a>本地运行</h2><p>Step 1：克隆项目。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/s1mplecc/external-link-crawer.git</span></span><br></pre></td></tr></table></figure>

<p>Step 2：安装依赖，包括 Flask 和 BeautifulSoup4。建议使用 Virtualenv 局部安装依赖。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip3 install -r requirements.txt</span> </span><br></pre></td></tr></table></figure>

<p>Step 3：在 IDE 中运行或通过 Flask 命令行工具启动应用，端口号默认为 5000。生产环境可使用 Gunicorn 部署。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> FLASK_ENV=<span class="string">&quot;development&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> flask run</span></span><br><span class="line"> * Environment: development</span><br><span class="line"> * Debug mode: on</span><br><span class="line"> * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure>

<h2 id="容器化部署"><a href="#容器化部署" class="headerlink" title="容器化部署"></a>容器化部署</h2><p>除本地运行之外，也支持在生产环境中使用容器化方式部署，镜像入口脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">gunicorn --worker-class=gevent --worker-connections=1000 -w 4 -b 0.0.0.0:8000 app:app</span><br></pre></td></tr></table></figure>

<p>使用 Gunicorn 启动 Flask 应用。由于爬虫运行效率主要受网络延迟影响，因此为提高并发吞吐量，使用多进程 + 协程方式部署。协程由 Gevent 库支持。参数 <code>-w</code> 指定进程数，每个进程默认最大并发连接数 1000。Gunicorn 应用端口号默认为 8000。</p>
<p>Step 1：拉取镜像。镜像已提交至 Docker Hub 仓库。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker pull s1mplecc/external-link-crawer</span></span><br></pre></td></tr></table></figure>

<p>也可以在本地手动构建镜像。克隆下项目后，在 Dockerfile 所在目录执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build -t s1mplecc/external-link-crawer .</span></span><br></pre></td></tr></table></figure>

<p>Step 2：启动容器，映射端口。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -d -p 5000:8000 --name external-link-crawer s1mplecc/external-link-crawer</span></span><br></pre></td></tr></table></figure>

<h2 id="请求格式"><a href="#请求格式" class="headerlink" title="请求格式"></a>请求格式</h2><ul>
<li>请求类型 GET；</li>
<li>请求前缀 <code>/external-link-domains</code> ；</li>
<li>参数 <code>url</code>，需传入合法 URL，否则返回参数异常状态码。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -XGET <span class="string">&quot;http://127.0.0.1:5000/external-link-domains?url=https://www.zhihu.com/&quot;</span></span></span><br></pre></td></tr></table></figure>

<h2 id="响应格式"><a href="#响应格式" class="headerlink" title="响应格式"></a>响应格式</h2><p>响应体为 JSON 格式，包含如下字段：</p>
<ul>
<li><code>data</code> 字段，数据本体；</li>
<li><code>code</code> 字段，状态码。成功 - 200，参数异常 - 400，服务器内部错误 - 500；</li>
<li><code>messages</code> 字段，附加消息。出错时提示异常信息。</li>
</ul>
<p>响应样例如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;code&quot;</span>: <span class="number">200</span>,</span><br><span class="line">    <span class="attr">&quot;data&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;css_scripts_domains&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;hm.baidu.com&quot;</span>,</span><br><span class="line">            <span class="string">&quot;static.zhihu.com&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;css_scripts_domains_size&quot;</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="attr">&quot;href_domains&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;app.mokahr.com&quot;</span>,</span><br><span class="line">            <span class="string">&quot;beian.miit.gov.cn&quot;</span>,</span><br><span class="line">            <span class="string">&quot;tsm.miit.gov.cn&quot;</span>,</span><br><span class="line">            <span class="string">&quot;www.12377.cn&quot;</span>,</span><br><span class="line">            <span class="string">&quot;www.beian.gov.cn&quot;</span>,</span><br><span class="line">            <span class="string">&quot;www.zhihu.com&quot;</span>,</span><br><span class="line">            <span class="string">&quot;zhstatic.zhihu.com&quot;</span>,</span><br><span class="line">            <span class="string">&quot;zhuanlan.zhihu.com&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;href_domains_size&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="attr">&quot;img_domains&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;pic2.zhimg.com&quot;</span>,</span><br><span class="line">            <span class="string">&quot;pic3.zhimg.com&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;img_domains_size&quot;</span>: <span class="number">2</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;messages&quot;</span>: <span class="string">&quot;[SUCCESS] ok&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参数异常响应样例如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;code&quot;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="attr">&quot;data&quot;</span>: <span class="literal">null</span>,</span><br><span class="line">    <span class="attr">&quot;messages&quot;</span>: <span class="string">&quot;[BAD_REQUEST] invalid param url value: xyz&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="镜像构建过程"><a href="#镜像构建过程" class="headerlink" title="镜像构建过程"></a>镜像构建过程</h2><p>Step 1：编写 Dockerfile。镜像基于 Python 官方镜像 3.8-slim 精简版本。将整个项目文件拷贝到镜像的工作目录内，在 <code>.dockerignore</code> 中指定忽略拷贝文件。入口文件为 Gunicorn 启动脚本。</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.8</span>-slim</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> FLASK_WORK_DIR=<span class="string">&quot;/root/flask&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> <span class="variable">$FLASK_WORK_DIR</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . .</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install -r requirements.txt</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip3 install gunicorn gevent</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chmod +x gunicorn_starter.sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">&quot;./gunicorn_starter.sh&quot;</span>]</span></span><br></pre></td></tr></table></figure>

<p>Step 2：构建镜像。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build -t s1mplecc/external-link-crawer .</span></span><br></pre></td></tr></table></figure>

<p>Step 3：推送到远端仓库。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker tag s1mplecc/external-link-crawer docker.io/s1mplecc/external-link-crawer</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker push docker.io/s1mplecc/external-link-crawer</span> </span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/" class="post-title-link" itemprop="url">基于 Elasticsearch 与 Kibana 搭建流量可视化平台</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-12-14 17:40:39" itemprop="dateCreated datePublished" datetime="2021-12-14T17:40:39+08:00">2021-12-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-17 13:47:55" itemprop="dateModified" datetime="2021-12-17T13:47:55+08:00">2021-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Ops/" itemprop="url" rel="index"><span itemprop="name">Ops</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>前期工作</strong>：基于 Bind9 搭建内网 DNS 服务器，使得在 <code>/etc/resolv.conf</code> 中配置了该服务器 IP 的节点或终端的 DNS 流量都会流经该服务器，为集群 DNS 流量采集提供了先决基础。同时，初步使用日志记录 DNS 查询请求（配置在 <code>/etc/named.conf</code> 中），后续可采用 Packetbeat/TShark 等工具主动捕获。DNS 查询日志内容格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">30-Nov-2021 08:57:25.192 client @0x7f74f800b650 192.168.101.145#61323 (apisix.apache.org): query: apisix.apache.org IN A + (168.168.168.47)</span><br><span class="line">30-Nov-2021 08:57:25.192 client @0x7f7514148d20 192.168.101.145#61325 (github.com): query: github.com IN A + (168.168.168.47)</span><br></pre></td></tr></table></figure>

<p>为了推进后续的 DNS 流量分类研究工作，本文介绍了如何基于现有的 DNS 流量日志，使用 Elasticsearch 和 Kibana 搭建流量监测可视化平台。本文包含以下内容：</p>
<ul>
<li>使用 <strong>Filebeat</strong> 将流量日志实时同步到 Elasticsearch；</li>
<li>使用 <strong>Grok</strong> 正则过滤器对采集到的日志消息二次处理；</li>
<li>使用网络数据包分析工具 <strong>Packetbeat</strong>。</li>
</ul>
<p>将日志文件转储到 Elasticsearch，一是作为历史归档考量，Bind9 日志存在存储时限，如果文件大小达到设置的上限，旧的日志文件会被循环覆盖。二是 JSON 格式易于拓展，支持嵌套和数组，且易于特征提取阶段 Python 进行处理。三是可视化支持好，在 Elasticsearch 数据集上，使用 Kibana 可以快速建立指标分析、图表分析以及时间序列分析等可视化。下图是基于 DNS 查询日志配置的 Kibana Dashboard 可视化界面。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/1.png" alt="93787acedd01f3ccf8ad7f2e4c62627d.png"></p>
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><ul>
<li>操作系统 CentOS 7.9；</li>
<li>Docker，版本 19.03.13；</li>
<li>Elastic Stack 工具集，版本 7.15.2。包括 Elasticsearch、Kibana、Filebeat 和 Packetbeat。</li>
</ul>
<h2 id="部署-Elasticsearch-Kibana"><a href="#部署-Elasticsearch-Kibana" class="headerlink" title="部署 Elasticsearch + Kibana"></a>部署 Elasticsearch + Kibana</h2><p>使用 Docker 快速部署 Elasticsearch 与 Kibana 环境。首先，拉取 Docker 镜像：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull docker.elastic.co/elasticsearch/elasticsearch:7.15.2</span><br><span class="line">$ docker pull docker.elastic.co/kibana/kibana:7.15.2</span><br></pre></td></tr></table></figure>

<p>需先以单点模式运行 Elasticsearch 容器，然后再启动 Kibana 容器。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --name es01-test --net elastic -p 9200:9200 -p 9300:9300 -e <span class="string">&quot;discovery.type=single-node&quot;</span> docker.elastic.co/elasticsearch/elasticsearch:7.15.2</span><br><span class="line">$ docker run -d --name kib01-test --net elastic -p 5601:5601 -e <span class="string">&quot;ELASTICSEARCH_HOSTS=http://es01-test:9200&quot;</span> docker.elastic.co/kibana/kibana:7.15.2</span><br></pre></td></tr></table></figure>

<p>成功启动容器后，可在 5601 端口访问 Kibana 首页。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/2.png" alt="d15ed7bec92f990449315ed49fa7bfeb.png"></p>
<h2 id="使用-Filebeat-监听日志文件变更"><a href="#使用-Filebeat-监听日志文件变更" class="headerlink" title="使用 Filebeat 监听日志文件变更"></a>使用 Filebeat 监听日志文件变更</h2><p>Filebeat 是 <a target="_blank" rel="noopener" href="https://www.elastic.co/cn/beats/">Elastic/Beats</a> 数据采集器工具集中的一员，负责日志采集，相比于运行在 JVM 上的 Logstash，它更加轻量级。Filebeat 能够实时捕获文件的新增内容（类似 <code>tail -f</code> 命令），按行将新增内容转发到 Elasticsearch 或 Logstash 中存储，并记录文件当前偏移量，以便中断后下次启动从中断点继续开始。</p>
<p>下载、解压 Filebeat 安装包并重命名。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.2-linux-x86_64.tar.gz</span><br><span class="line">$ tar -xzvf filebeat-7.15.2-linux-x86_64.tar.gz </span><br><span class="line">$ mv filebeat-7.15.2-linux-x86_64 filebeat</span><br></pre></td></tr></table></figure>

<p>Filebeat 安装目录中，包含启动脚本以及默认配置文件 filebeat.yml。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> filebeat</span><br><span class="line">$ ./filebeat version</span><br><span class="line">filebeat version 7.15.2 (amd64), libbeat 7.15.2 [fd322dad6ceafec40c84df4d2a0694ea357d16cc built 2021-11-04 14:22:49 +0000 UTC]</span><br></pre></td></tr></table></figure>

<p>按照 filebeat.yml 文件格式，编写<strong>自定义配置文件</strong> named.yml。在配置文件中，设置日志输入源为 Bind9 生成的 query.log；去除了 Filebeat 生成的冗余字段（可选）；设置了写入的 Elasticsearch 的 URL 以及索引分片数。</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">filebeat.inputs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">type:</span> <span class="string">log</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">paths:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/var/log/named/query.log</span></span><br><span class="line"></span><br><span class="line"><span class="attr">processors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">drop_fields:</span></span><br><span class="line">      <span class="attr">fields:</span> [<span class="string">&quot;host&quot;</span>,<span class="string">&quot;agent&quot;</span>,<span class="string">&quot;ecs&quot;</span>,<span class="string">&quot;input&quot;</span>]</span><br><span class="line">      <span class="attr">ignore_missing:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">setup.template.settings:</span></span><br><span class="line">  <span class="attr">index.number_of_shards:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">output.elasticsearch:</span></span><br><span class="line">  <span class="attr">hosts:</span> [<span class="string">&quot;http://localhost:9200&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>默认情况下，Filebeat 写入 Elasticsearch 的索引名形如 <code>filebeat-%&#123;[beat.version]&#125;-%&#123;+yyyy.MM.dd&#125;</code>。此项配置在 <code>output.elasticsearch.index</code> 中，Filebeat 会自动创建索引别名，如 filebeat-7.15.2。</p>
<p>运行 Filebeat，参数 <code>-c</code> 指定配置文件 named.yml，<code>-e</code> 将错误日志输出到控制台。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./filebeat -e -c named.yml</span><br></pre></td></tr></table></figure>

<p>注：使用 <code>nohup</code> 和 <code>&amp;</code> 命令可以后台运行，并且输入 <code>exit</code> 退出命令行终端后依然不会停止（直接关闭命令行终端会停止）。</p>
<p>从 Filebeat 发往 Elasticsearch 的日志消息，包装在 message 字段中，除此之外，还有一些 Filebeat 生成的记录字段。一条日志信息转化后的格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_index&quot;</span>: <span class="string">&quot;filebeat-7.15.2-2021.11.26-000001&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_type&quot;</span>: <span class="string">&quot;_doc&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_id&quot;</span>: <span class="string">&quot;5qjyan0BAsnp-ar_z6EP&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;_version&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;_score&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;_source&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;@timestamp&quot;</span>: <span class="string">&quot;2021-11-29T09:07:16.390Z&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;log&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;offset&quot;</span>: <span class="number">2892475</span>,</span><br><span class="line">      <span class="attr">&quot;file&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;path&quot;</span>: <span class="string">&quot;/var/log/named/query.log&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;message&quot;</span>: <span class="string">&quot;29-Nov-2021 17:07:13.335 client @0x7fd6a0010f10 192.168.101.60#58832 (shcn2-06b.iplc188.com): query: shcn2-06b.iplc188.com IN A + (168.168.168.47)&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;fields&quot;</span>: &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于原始的 message 字段，能够提供的信息有限，因此需要对消息字段加以处理。Logstash 的插件 Grok 正则过滤器提供对字符串的解析功能。如今，Elastic 提供了 Ingest Node 功能，允许在文档写入 Elasticsearch 索引之前经过<strong>预处理</strong>，并提供了对 Grok 的集成。</p>
<h2 id="使用-Grok-正则处理日志消息"><a href="#使用-Grok-正则处理日志消息" class="headerlink" title="使用 Grok 正则处理日志消息"></a>使用 Grok 正则处理日志消息</h2><p>在 Kibana 的 “Stack Management | Ingest Node Pipelines” 界面，可以指定一系列的处理器 （Processors）对消息进行流水线处理，包括定义异常处理分支。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/3.png" alt="dc741da57eca3e2b9bdc4bf70357cc3c.png"></p>
<p>对于 query.log 文件中的日志记录，由于日志格式固定，因此可以使用 Grok 对一条记录进行匹配。正则表达式格式如 <code>%&#123;IP:source.ip&#125;</code>，冒号左侧为 <a target="_blank" rel="noopener" href="https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/legacy/grok-patterns">Grok 默认 Pattern</a>，冒号右侧为存储的 JSON 字段名，支持嵌套结构。解析日志的 Grok 正则表达式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">23-Nov-2021 13:58:03.222 client @0x7f7034054680 168.168.168.47#56504 (baidu.com): query: baidu.com IN A + (168.168.168.47)</span><br><span class="line"></span><br><span class="line">%&#123;GREEDYDATA:timestamp&#125; %&#123;WORD&#125; %&#123;DATA:source.client&#125; %&#123;IP:source.ip&#125;#%&#123;NUMBER:source.port&#125; \(%&#123;HOSTNAME:domain.fullname&#125;\): query: %&#123;HOSTNAME&#125; %&#123;WORD:class&#125; %&#123;WORD:type&#125; </span><br></pre></td></tr></table></figure>

<p>此外，还可以<strong>自定义 Grok Pattern</strong>。例如，对于域名字段来说，其“二级域名 + 顶级域名”构成的域名后缀（即用户可以在域名注册网站购买的域名）可能具有重要分析意义。为此，定义了名为 SLD 的  Grok Pattern，将其解析成 <code>domain.eTLDplusOne</code> 字段（有效顶级域名 + 1，名称借鉴自 Packetbeat）。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/4-1.png" alt="fb23cc0fd30fb9e1e9c8d89ee52a22a6.png"></p>
<p>注：正则表达式中的反义字符由于要表示在 JSON 文档中，所以需要额外一个下划线，如 <code>\\.</code>。</p>
<p>可以在 “Stack Management | Ingest Node Pipelines” 界面添加测试文档测试流水线是否正常工作，这对于验证正则表达式正确性有巨大帮助。此外，还能够在 Dev Tools 的 Grok Debugger 控制台中调试正则表达式。</p>
<p>实际上，在 Kibana 界面添加 Pipeline 相当于在 Elasticsearch 的 <code>/_ingest/pipeline</code> 索引中添加了如下的一条文档：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;dns_query_log_pipeline&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;description&quot;</span> : <span class="string">&quot;Extract fields from message.&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;processors&quot;</span> : [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;grok&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;field&quot;</span> : <span class="string">&quot;message&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;patterns&quot;</span> : [</span><br><span class="line">            <span class="string">&quot;&quot;</span><span class="string">&quot;%&#123;GREEDYDATA:timestamp&#125; %&#123;WORD&#125; %&#123;DATA:source.client&#125; %&#123;IP:source.ip&#125;#%&#123;NUMBER:source.port&#125; \(%&#123;HOSTNAME:domain.fullname&#125;\): query: %&#123;HOSTNAME&#125; %&#123;WORD:class&#125; %&#123;WORD:type&#125; &quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;date&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;field&quot;</span> : <span class="string">&quot;timestamp&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;formats&quot;</span> : [</span><br><span class="line">            <span class="string">&quot;dd-MMM-yyyy HH:mm:ss.SSS&quot;</span></span><br><span class="line">          ],</span><br><span class="line">          <span class="attr">&quot;target_field&quot;</span> : <span class="string">&quot;timestamp&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;grok&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;field&quot;</span> : <span class="string">&quot;domain.fullname&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;patterns&quot;</span> : [</span><br><span class="line">            <span class="string">&quot;&quot;</span><span class="string">&quot;(%&#123;GREEDYDATA:domain.prefix&#125;\.)*(%&#123;SLD:domain.eTLDplusOne&#125;)&#123;1&#125;&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">          ],</span><br><span class="line">          <span class="attr">&quot;pattern_definitions&quot;</span> : &#123;</span><br><span class="line">            <span class="attr">&quot;SLD&quot;</span> : <span class="string">&quot;&quot;</span><span class="string">&quot;((xn--)?[a-z0-9-]+\.[a-z|\-]&#123;2,63&#125;)&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">&quot;on_failure&quot;</span> : [</span><br><span class="line">            &#123;</span><br><span class="line">              <span class="attr">&quot;set&quot;</span> : &#123;</span><br><span class="line">                <span class="attr">&quot;field&quot;</span> : <span class="string">&quot;domain.hostname&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;copy_from&quot;</span> : <span class="string">&quot;domain.fullname&quot;</span></span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为了使该 Pipeline 生效，还需要在 Filebeat 的配置文件中添加此项配置，并重启 Filebeat 程序。</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">output.elasticsearch:</span></span><br><span class="line">  <span class="attr">hosts:</span> [<span class="string">&quot;http://localhost:9200&quot;</span>]</span><br><span class="line">  <span class="attr">pipeline:</span> <span class="string">&quot;dns_query_log_pipeline&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="Kibana-可视化"><a href="#Kibana-可视化" class="headerlink" title="Kibana 可视化"></a>Kibana 可视化</h2><p>要在 Kibana 查看 Elasticsearch 中存储的数据，首先需要在 “Stack Management | Index patterns” 中配置索引模式（Index pattern）。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/4-2.png" alt="c1ddec742b59b52562067be4e9ed6231.png"></p>
<p>创建完成后，可以在 “Discover” 界面查看索引中存放的数据。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/5.png" alt="6625242b24495aaf1c3a084b4c17bb71.png"></p>
<p>还可以在 “Dashboard” 界面自定义可视化看板，效果图如前言中所示。</p>
<h2 id="使用-Packetbeat-监听数据包"><a href="#使用-Packetbeat-监听数据包" class="headerlink" title="使用 Packetbeat 监听数据包"></a>使用 Packetbeat 监听数据包</h2><p>除了使用 Filebeat + Grok 手动解析日志文件之外，Beats 工具集还提供了专用于<strong>网络数据包分析</strong>的轻量型工具 Packetbeat，它能够捕获由 ping 工具发起的 ICMP 报文（网络层），或者通过端口捕获应用层报文，如 HTTP 80 端口、MySQL 3306 端口等。捕获的报文可被解析成 JSON 格式存储到 Elasticsearch。</p>
<p>下载、解压 Packetbeat 安装包，并重命名。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl -L -O https://artifacts.elastic.co/downloads/beats/packetbeat/packetbeat-7.15.2-linux-x86_64.tar.gz</span><br><span class="line">$ tar xzvf packetbeat-7.15.2-linux-x86_64.tar.gz</span><br><span class="line">$ mv packetbeat-7.15.2-linux-x86_64 packetbeat</span><br></pre></td></tr></table></figure>

<p>Packetbeat 的配置文件模版位于 packetbeat.yml 中，我们需要自定义 DNS 配置文件 dns.yml。除了以 <code>packetbeat.</code> 开头的特定配置外，其他配置与 Filebeat 共用一套模版。</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">packetbeat.interfaces.device:</span> <span class="string">eth0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">packetbeat.protocols:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">type:</span> <span class="string">dns</span></span><br><span class="line">  <span class="attr">ports:</span> [<span class="number">53</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">processors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">drop_fields:</span></span><br><span class="line">      <span class="attr">fields:</span> [<span class="string">&quot;host&quot;</span>,<span class="string">&quot;agent&quot;</span>,<span class="string">&quot;ecs&quot;</span>]</span><br><span class="line">      <span class="attr">ignore_missing:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">setup.template.settings:</span></span><br><span class="line">  <span class="attr">index.number_of_shards:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">output.elasticsearch:</span></span><br><span class="line">  <span class="attr">hosts:</span> [<span class="string">&quot;http://localhost:9200&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>运行 Packetbeat，参数 <code>-c</code> 指定配置文件 named.yml，<code>-e</code> 将错误日志输出到控制台。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./packetbeat -e -c dns.yml</span><br></pre></td></tr></table></figure>

<p>启动 Packetbeat 后，捕获的数据被发送至 Elasticsearch，在 “Stack Management | Index patterns” 中配置索引模式后，就可以访问以 packetbeat-7.15.2 别名命名的索引中的数据。</p>
<p>此外，Packetbeat 还提供了一些可视化的 Kibana Dashboard 模版，通过如下命令初始化：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./packetbeat setup --dashboards</span><br></pre></td></tr></table></figure>

<p>Dashboard 效果图如下。</p>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/6.png" alt="7a2a13ab1327c49fa654607b52f662e5.png"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>值得注意的是，Packetbeat 监听的是目的端口为 53 的数据包。不管是下面哪种 DNS 报文，Packetbeat 都能进行捕获。</p>
<ul>
<li>内网中访问内网 DNS 服务器的其他终端，如 192.168.101.215 -&gt; 168.168.168.47（内网 DNS 服务器）；</li>
<li>内网 DNS 服务器转发的 DNS 查询，如 168.168.168.47 -&gt; 8.8.8.8（谷歌 DNS 服务器，可在 Bind9 配置文件中定义转发规则）；</li>
<li>甚至是手动指定 DNS 服务器的查询报文，如 <code>dig @114.114.114.114 baidu.com</code>。</li>
</ul>
<p><img data-src="/2021/12/14/%E5%9F%BA%E4%BA%8E-Elasticsearch-%E4%B8%8E-Kibana-%E6%90%AD%E5%BB%BA%E6%B5%81%E9%87%8F%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0/7.png" alt="08d3eb68410326d6b2798b8aa397eacc.png"></p>
<p>相比之下，使用日志记录 DNS 查询请求的方式就显得十分局限了。即使内网中的用户在 <code>/etc/resolv.conf</code> 中配置了内网 DNS 服务器，也不能保证恶意软件乖乖的使用该 DNS 服务器。因此理论上，只有在内网所有终端上部署类似 Packetbeat 的数据包探测器（类似的还有 Wireshark 的底层库工具 tshark），才能真正全量的监听 DNS 流量。此外，Packetbeat 解析出的 JSON 结构的数据所包含的信息要远大于通过正则分析的日志信息。</p>
<p>因此对于端口固定的数据包探测，Packetbeat 的实用性要优于 Filebeat + Grok 的方案。但 Filebeat 适用范围更广，任何有生成日志的程序，甚至是批/流处理产生的中间数据文件，理论上都能通过 Filebeat 采集特征存入 Elasticsearch 进行后续分析。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/filebeat/current/configuring-ingest-node.html#configuring-ingest-node">Filebeat Reference | Parse data by using ingest node</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/legacy/grok-patterns">logstash-plugins/grok-patterns</a></li>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/beats/packetbeat/current/packetbeat-overview.html">Packetbeat Reference | Packetbeat overview</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">结合源码深入理解 MapReduce 工作原理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-22 15:36:44" itemprop="dateCreated datePublished" datetime="2021-10-22T15:36:44+08:00">2021-10-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-17 13:47:55" itemprop="dateModified" datetime="2021-12-17T13:47:55+08:00">2021-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Concepts/" itemprop="url" rel="index"><span itemprop="name">Concepts</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="MapReduce-架构"><a href="#MapReduce-架构" class="headerlink" title="MapReduce 架构"></a>MapReduce 架构</h2><blockquote>
<p>本文讨论的 MapReduce 架构是 Hadoop 1.0 版本时的架构，从 Hadoop 2.0 开始，Hadoop 推出了资源管理框架 YARN。在 YARN 中，使用 ResourceManager 来负责容器的调度（任务运行在容器中），以及作业的管理。使用 NodeManager 来向 ResourceManager 汇报节点资源以及容器运行状态，NodeManager 负责创建并管理执行任务的容器。从职责方面，ResourceManager 等同于 JobTracker，NodeManager 等同于 TaskTracker。大多数的分布式框架都符合这种主从设计，如 HDFS 的 NameNode 和 DataNode、Spark 的 Driver 和 Executor 等。</p>
</blockquote>
<p>以下是 MapReduce 架构和工作流程中的常用术语：</p>
<ul>
<li>Job（作业）：是客户端需要执行的一个工作单元，包括输入数据、MapReduce 程序和配置信息；</li>
<li>Task（任务）：Hadoop 将作业分成若干个任务来执行，任务分为两类，即 Map 任务和 Reduce 任务；</li>
<li>Map/Reduce：从执行阶段来看，Map 和 Reduce 代表两个大类阶段。从计算模型角度看，它们代表两个计算步骤。从代码层面看，它们是定义在 Mapper 和 Reducer 类中的函数；</li>
<li>Mapper/Reducer：从执行阶段的详细划分来看，Mapper 和 Reducer 代表执行 map 与 reduce 函数的步骤。从代码层面看，这是定义在代码中的两个 Java 类。某些语境下，Mapper 可以指代 Map 任务，Reducer 同理。</li>
</ul>
<p>与多数的大数据分布式框架相同，MapReduce 的架构也遵循主从结构：</p>
<ul>
<li>运行在 HDFS NameNode 主节点上的 <strong>JobTracker</strong> 程序，负责接收从客户端提交的 Job，将其划分成 Map 任务和 Reduce 任务，分发给从节点 TaskTracker 执行。JobTracker 负责任务之间的协作，并通过 TaskTracker 发送来的心跳包维护集群的运行状态，以及作业进度信息。</li>
<li>多个运行在 HDFS DataNode 节点上的 <strong>TaskTracker</strong> 程序，负责执行 Map 任务和 Reduce 任务，直接与 HDFS 交互。每隔一段时间，TaskTracker 向 JobTracker 发送心跳包，汇报节点运行状态，以及任务完成进度。</li>
</ul>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/1.png" alt="66b01a299a89595ca02de7b5b9f02b85.png"></p>
<p>为了减少网络传输带来的性能影响，JobTracker 在分发 Map 任务时基于<strong>数据本地化优化</strong>（Data locality optimization）策略，将 Map 任务分发给包含此 Map 处理数据的从节点，并将程序 Jar 包发送给该 TaskTracker，遵循“运算移动，数据不移动”的原则。</p>
<h2 id="新旧版本-API-说明"><a href="#新旧版本-API-说明" class="headerlink" title="新旧版本 API 说明"></a>新旧版本 API 说明</h2><p>本文中的源码是 Hadoop 3.2.0 版本的源码，使用的是 Hadoop 新版 API。新版 API 位于 <code>org.apache.hadoop.mapreduce</code> 包下，旧版 API 位于 <code>org.apache.hadoop.mapred</code> 包下，两版 API 并不兼容，你可以在 <a target="_blank" rel="noopener" href="https://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api">这里</a> 查看两者的区别。为保证向后兼容，Hadoop 并没有移除旧版 API，因此依赖库中两个版本并存，使用时要注意。</p>
<p>设计新版 API 的主要目的是给用户提供更加简洁优雅的接口，框架的核心代码并没有调整包，比如执行 Map 与 Reduce 任务的 MapTask 和 ReduceTask 类仍位于 <code>org.apache.hadoop.mapred</code> 包下。但由于使用了两套 API，在该类中你会经常看到以 Old/New 命名的类或方法，如 <code>runNewMapper()</code> 和 <code>runOldMapper()</code>。</p>
<p>在旧版本中，Mapper 与 Reducer 被定义为接口，而在新版本中被定义为具体类，并提供默认的 map 和 reduce 实现：仅是通过 <code>context.write()</code> 重新写回数据。旧版本的 Mapper 接口源码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Mapper</span>&lt;<span class="title">K1</span>, <span class="title">V1</span>, <span class="title">K2</span>, <span class="title">V2</span>&gt; <span class="keyword">extends</span> <span class="title">JobConfigurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">map</span><span class="params">(K1 key, V1 value, OutputCollector&lt;K2, V2&gt; output, Reporter reporter)</span> <span class="keyword">throws</span> IOException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>新版本中广泛使用上下文 Context 对象，整合了 OutputCollector、Reporter  和部分 JobConf 的功能，来实现与 MapReduce 系统的通信，如使用 <code>context.write()</code> 代替旧版的 <code>output.collect()</code> 功能。</p>
<p>新版本使用 Job 类替换旧版的 JobClient 类，实现作业控制，并使用统一的配置类 Configuration 来替换旧版的 JobConf 类，用于作业配置。因此，现在的一个 MapReduce 程序的入口函数可能如下所示：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">    Job job = Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>);</span><br><span class="line">    job.setJarByClass(WordCount.class);</span><br><span class="line">    </span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">    </span><br><span class="line">    job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">    job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">    job.setReducerClass(IntSumReducer.class);</span><br><span class="line">    </span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    </span><br><span class="line">    System.exit(job.waitForCompletion(<span class="keyword">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce-中的数据格式"><a href="#MapReduce-中的数据格式" class="headerlink" title="MapReduce 中的数据格式"></a>MapReduce 中的数据格式</h2><p>MapReduce 工作过程中每个阶段的输入和输出数据都是以<strong>键值对</strong>的形式出现，如下表所示，表中的 k 与 v 代表了数据类型，不同下标代表不同的数据类型。</p>
<table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">输入</th>
<th align="center">输出</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>map()</code></td>
<td align="center">&lt;$k_1$, $v_1$&gt;</td>
<td align="center">[&lt;$k_2$, $v_2$&gt;, …]</td>
<td align="center">一个输入分片被 Map 处理成一系列的键值对</td>
</tr>
<tr>
<td align="center"><code>reduce()</code></td>
<td align="center">&lt;$k_2$, [$v_2$, …]&gt;</td>
<td align="center">&lt;$k_3$, $v_3$&gt;</td>
<td align="center">Reduce 的输入键类型为 Map 后的输出键类型，输入值是键对应的值的集合</td>
</tr>
</tbody></table>
<p>下面以统计词频程序 WordCount 为例，来说明 MapReduce 过程中的数据类型转换。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>在 Mapper 工作之前，框架从文件系统中读取文件并切分为分片，将数据转换为 <code>&lt;KEYIN, VALUEIN&gt;</code> 格式的键值对传给 map 函数。在 WordCount 程序中可以理解为形如 <code>&lt;行号, &quot;a b c&quot;&gt;</code> 的输入数据。实际上，输入键“行号”在实际代码中可能如下的 LongWritable 类型，一个可序列化的长整型偏移量（offset）。这是由 Hadoop 框架定义的数据类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] words = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            Text outputKey = <span class="keyword">new</span> Text(word.toUpperCase().trim());</span><br><span class="line">            IntWritable outputValue = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">            context.write(outputKey, outputValue);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：Hadoop 提供了一套可优化网络序列化传输的基本类型，而不直接使用 Java 内置类型，这些类型位于 <code>org.apache.hadoop.io</code> 包中。上述代码中的 LongWritable 相当于 Java 的 Long 类型，Text 相当于 String 类型，IntWritable 相当于 Integer 类型。</p>
<p>map 函数将输入的文本串切分为一个个单词，输入的键值对被转换为 <code>&lt;KEYOUT, VALUEOUT&gt;</code> 格式的<strong>中间键值对</strong>输出。此时的输出键为单词，输出值为单词计数，默认为 1。即经由 Mapper 处理后，原本的“一行”数据被转换为了形如 <code>[&lt;&quot;a&quot;, 1&gt;, &lt;&quot;b&quot;, 1&gt;, &lt;&quot;c&quot;, 1&gt;]</code> 的中间数据。</p>
<p>中间数据是暂时数据，不会存入 HDFS，但是会存入运行 Map 任务节点的本地磁盘，经过数据混洗后被 Reducer 端消费。</p>
<h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><p>由于 Reduce 任务与 Map 任务与不一定处于同一节点上，Reduce 任务会通过网络通信拉取多个 Map 任务产生的中间数据。数据从 Map 任务端传输给 Reduce 任务端的过程被称为<strong>数据混洗</strong>。</p>
<p>混洗之后传入给 Reducer 的输入键值对的值，是该键对应的值的集合（可迭代对象），如 <code>&lt;&quot;a&quot;, [1, 2, 1]&gt;</code>。值可能为 2 是因为，为了减少磁盘写入和网络传输的数据量，Map 任务可能会在本地节点上预先聚合，这也就是 Combiner 所做的工作。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text word, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(word, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>经过 reduce 函数处理后，单词的所有计数值被累加，输出形如 <code>&lt;&quot;a&quot;, 4&gt;</code> 的键值对。</p>
<h2 id="MapReduce-工作流程详解"><a href="#MapReduce-工作流程详解" class="headerlink" title="MapReduce 工作流程详解"></a>MapReduce 工作流程详解</h2><p>MapReduce 将处理过程分成两个大类阶段：Map 阶段和 Reduce 阶段。在 Map 阶段，由于任务分发基于数据本地化原则，Map 任务运行在包含有该任务处理数据的节点上，数据存储在当前节点的 HDFS DataNode 中。因此，Map 阶段处理的都是本地数据，不需要进行网络传输。Map 阶段产生的中间数据将会暂存在当前节点上，Reduce 阶段需要从相关节点上拉取数据进行聚合运算，再将结果写入 HDFS。因此，Reduce 阶段既需要磁盘读写，也需要网络传输。</p>
<p>当然，MapReduce 在工作时还要细分为许多小阶段，下面这张图很好的展示了 MapReduce 的整个工作流程，具体包括：数据读入阶段、Mapper 处理阶段、优化性阶段 Combiner、中间数据分区 Partitioner 阶段、被称为 MapReduce “心脏”的数据混洗 Shuffle 阶段、Reducer 处理阶段以及数据写入阶段。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/2.jpg" alt="9b17e681d1c5745840c517209dd2808f.jpeg"></p>
<p>从大类阶段上划分，数据混洗之前的阶段都可以划分到 Map 阶段，因为这些步骤都是在本地节点上完成的，不涉及网络传输。而在数据混洗阶段，程序从本地或是集群上的其他节点拉取并拷贝 Mapper 产生的中间数据，提供给 Reducer 作为输入。因此，可以将包括数据混洗及其之后的步骤划分为 Reduce 阶段。事实上，数据混洗的相关类也被定义在 <code>org.apache.hadoop.mapreduce.task.reduce</code> 包下。</p>
<p>下面将结合源码详细介绍各个阶段的工作。</p>
<h3 id="数据读入阶段"><a href="#数据读入阶段" class="headerlink" title="数据读入阶段"></a>数据读入阶段</h3><p>数据被 Mapper 处理前，需要先转换为 Mapper 支持的键值对类型，这个过程由 InputFormat 和 RecordReader 类完成。首先，由 InputFormat 类从 HDFS 读入文件并创建<strong>输入分片</strong>（Input Split），分片为等长的逻辑数据块，比如可能是形如 <code>(input-file-path, start, offset)</code> 的元组。</p>
<p>一个合理的分片大小应该与 HDFS 块大小保持一致，默认为 128 MB。<strong>Hadoop 会在分片数据所在的物理节点寻找一个空闲的 Map 槽，运行 Map 任务</strong>，由该任务运行用户自定义的 map 函数从而处理分片中的每条记录。此时，符合之前所说的数据本地化原则。一旦一个分片跨越两个物理块，由于 HDFS 的分布式存储特性，这两个块极可能位于不同的 DataNode 上，此时分片中的部分数据就需要通过网络传输到 Map 任务运行的节点。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">InputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> RecordReader&lt;K, V&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>InputFormat 会为一个分片创建一个 RecordReader 对象，负责将输入分片转换为 Mapper 可处理的键值对。从数据视图角度看，输入数据字节流被转换为了面向记录的视图。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordReader</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>&gt; <span class="keyword">implements</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> KEYIN <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> VALUEIN <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Mapper-阶段"><a href="#Mapper-阶段" class="headerlink" title="Mapper 阶段"></a>Mapper 阶段</h3><p>在这个阶段，Mapper 会执行用户定义的 map 函数，并将输入的键值对转换为中间键值对序列。在 MapTask 类中，使用新版本的 Mapper 会直接调用 <code>mapper.run()</code> 方法运行，旧版本还需要由包装有 Mapper 的 MapRunner 来运行。新版的 API 中，允许用户覆盖 <code>run()</code> 方法，以及 Context 对象的生命周期方法，来做到对 Mapper 执行的完全控制。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mapper</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(KEYIN key, VALUEIN value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        setup(context);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (context.nextKeyValue()) &#123;</span><br><span class="line">                map(context.getCurrentKey(), context.getCurrentValue(), context);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            cleanup(context);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注：Map 阶段的整个工作流程，可以在 MapTask 类的 <code>run()</code> 和 <code>runNewMapper()</code> 或 <code>runOldMapper()</code> 方法中清晰的一览。</p>
<h3 id="Partitioner-阶段"><a href="#Partitioner-阶段" class="headerlink" title="Partitioner 阶段"></a>Partitioner 阶段</h3><p>与 Map 任务不同，Reduce 任务并不具备数据本地化的优势，单个 Reduce 任务的输入通常来自于所有 Mapper 的输出。因此，即使 Reduce 任务与某些个 Map 任务处于同一节点上，也不可避免的需要通过网络传输从其他节点上获取 Mapper 输出。</p>
<p>当只有一个 Reduce 任务时，这个 Reduce 任务会读取所有 Mapper 输出，此时对中间数据分区意义不大，因为所有 Mapper 输出都被写入同一文件。但当有多个 Reduce 任务时，为提高数据吞吐量，<strong>每个 Map 任务会针对输出进行分区（Partition），即为每个 Reduce 任务创建一个分区</strong>。同一个键对应的键值对记录都被划分在同一分区中，每个分区中可以包含许多键（及其对应的值）。并且，一个分区中的记录是<strong>按键排序</strong>的，这样，磁盘读取一个键的所有记录时能保证读取连续数据，而不是从零散的文件中再过滤数据。</p>
<h4 id="数据溢写"><a href="#数据溢写" class="headerlink" title="数据溢写"></a>数据溢写</h4><p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/3.png" alt="20cbf28f315007e05a1c762915a25ca2.png"></p>
<p>每个 Map 任务有一个环形内存缓冲区，缓冲区大小由 <code>io.sort.mb</code> 指定，默认 100MB。Mapper 产生的中间键值对记录将被先写入缓冲区，当达到缓冲区设定阈值时（<code>io.sort.spill.percent</code>，默认 80%），会开启一个线程将内容<strong>溢写</strong>（spill）到磁盘，由 <code>mapred.local.dir</code> 属性指定的目录。线程工作的同时，Mapper 的输出继续被写到缓冲区，如果在此期间缓冲区被写满，Mapper 将会阻塞直到溢写过程结束。</p>
<p><strong>每次缓冲区达到溢出阈值，就会新建一个溢出文件，键值对会在内存中先按键排序然后写入文件</strong>。在任务完成前，溢出文件会不断合并并保证文件中数据是有序的。</p>
<p>数据溢写会写入运行 Map 任务节点的本地磁盘，并不会写入 HDFS（但 Reducer 的输出并不是这样）。这是因为，Map 任务运行的输入分片来自本地节点，输出也应写入本地节点，这样保证了整个 Map 任务的数据本地化。而写入 HDFS 则会为数据创建分布式副本，带来额外网络开销。更何况，Map 任务产生的输出只是暂时数据，任务执行完毕后会被删除。</p>
<h4 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h4><p>下面我们通过阅读 <code>org.apache.hadoop.mapred.MapTask</code> 类的相关源码，来加深对 Map 任务将中间键值对分区、排序存入磁盘过程的理解。Hadoop 使用 OutputCollector 将 Mapper 数据写入磁盘，由于继承/实现的类不同，MapTask 类中存在 Old/NewOutputCollector 两套新旧收集器。以新版 NewOutputCollector 为例，它的实现逻辑如下：</p>
<ul>
<li>首先，创建一个排序收集器 collector，具体的排序逻辑在 MapOutputBuffer 类中，其实现了 IndexedSortable 接口的 <code>compare()</code> 方法，按键进行排序；</li>
<li>从作业上下文（配置）中获取 Reduce 任务总数；</li>
<li>当 Reduce 任务数大于一时，通过反射创建 Partitioner 类的实例，这个类可以是用户自定义的类，并在配置阶段注入；</li>
<li>如果 Reduce 任务数等于零（纯 Map 作业）或等于一，创建匿名内部类，返回的分区号为固定值，所有输出写到同一分区；</li>
<li>写入阶段通过收集器的 <code>collect()</code> 方法，将键值对按照分区号写入对应分区。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">NewOutputCollector</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> MapOutputCollector&lt;K,V&gt; collector;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Partitioner&lt;K,V&gt; partitioner;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partitions;</span><br><span class="line"></span><br><span class="line">    NewOutputCollector(JobContext jobContext, JobConf job, TaskUmbilicalProtocol umbilical, TaskReporter reporter) <span class="keyword">throws</span> IOException, ClassNotFoundException &#123;</span><br><span class="line">        collector = createSortingCollector(job, reporter);</span><br><span class="line">        partitions = jobContext.getNumReduceTasks();</span><br><span class="line">        <span class="keyword">if</span> (partitions &gt; <span class="number">1</span>) &#123;</span><br><span class="line">            partitioner = (Partitioner&lt;K,V&gt;) ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partitioner = <span class="keyword">new</span> Partitioner&lt;K,V&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">return</span> partitions - <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(K key, V value)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        collector.collect(key, value, partitioner.getPartition(key, value, partitions));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Partitioner 类位于 <code>org.apache.hadoop.mapreduce</code> 包下，其作用是返回一个整型分区号，Map 任务将这个分区号作为写入哪个分区的标识。用户可以自定义分区函数，需要继承 Partitioner 类。通常，默认的分区函数 HashPartitioner 足够用了，它使用哈希函数，将键进行哈希后取非负（前置位补零）然后对 Reduce 任务总数取模。这样，能保证键相同的记录被分配至同一分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="comment">/** Use &#123;<span class="doctag">@link</span> Object#hashCode()&#125; to partition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value, <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HashPartitioner 足够高效，但如果你执行的 MapReduce 作业发生数据倾斜的问题（如存在大部分相同键），可以考虑自定义分区函数，比如加入随机值。</p>
<h4 id="Reduce-任务数设置"><a href="#Reduce-任务数设置" class="headerlink" title="Reduce 任务数设置"></a>Reduce 任务数设置</h4><p>Hadoop MapReduce 的<strong>并行度</strong>取决于 Map 任务数量和 Reduce 任务数量。Map 任务数量不需要手动设置，原因是该数量等于输入文件被划分成的分片数，框架会为一个输入分片分配一个 Map 任务。Map 任务数据取决于输入文件的大小以及 HDFS 文件块的大小。默认情况下，输入分片大小与块大小保持一致，均为 128 MB。</p>
<p>Reduce 任务数量可以在作业配置时通过 <code>job.setNumReduceTasks()</code> 手动设置。默认情况下，只有一个 Reduce 任务，这对于本地小规模数据已经足够了。但在真实应用中，几乎所有作业都将它设置为一个较大的数字，否则，所有的中间数据都被传输给一个 Reduce 任务，作业处理极其低效。</p>
<p>为一个作业设置多少个 Reduce 任务数，与其说是一门技术，不如说更多是一门艺术。由于并行化程度提高，增加 Reducer 能缩短 Reduce 阶段整体耗时。并且，增加的 Reducer 对于解决数据倾斜问题通常能有很好的帮助。然而，如果配置了过多 Reducer，文件将被切分为更多小文件，磁盘 I/O 次数将显著增加，这又不够优化。相对于大批量的小文件，Hadoop 更适合处理少量的大文件。一条经验法则是，目标 Reducer 保持在每个运行 5 分钟左右，且产生至少一个 HDFS 块的输出比较合适。</p>
<h3 id="Combiner-阶段"><a href="#Combiner-阶段" class="headerlink" title="Combiner 阶段"></a>Combiner 阶段</h3><p>为了减少 Map 和 Reduce 任务之间的数据传输量，Hadoop 允许用户针对 Map 任务指定一个 Combiner 类。以统计词频程序为例，Mapper 输出的键值对 <code>&lt;单词，计数值&gt;</code> 的计数值初始均为一，此时可通过 Combiner 将相同键的值进行聚合，即计数值累加。这样，同一个 Mapper （来自同一输入分片）的处理数据将被提前聚合，既减少了磁盘写入数据量，也减少了需要通过网络传输给 Reducer 的数据量。因此 Combiner 也被称为 “Mini-reducer” 或 “Local-reducer”，意指在本地节点完成的。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/4.png" alt="e58767ee9dfc6d8bf818f639a0508b7f.png"></p>
<p>Combiner 工作在文件溢写的前后，具体是在 MapTask 的内部类的 <code>sortAndSpill() </code> 和 <code>mergeParts()</code> 方法中，由 <code>combinerRunner.combine()</code> 运行。在 Mapper 输出的键值对被溢写到磁盘之前，会在内存中按键排序，如果定义了 Combiner，它将在排序后的输出上运行，经过 Combiner 紧凑后的数据再写入磁盘。在溢出文件合并阶段，如果至少存在 3 个溢出文件（由 <code>mapreduce.map.combine.minspills</code> 属性指定），那么 Combiner 将会在文件合并时再次运行。因此，Combiner 可能会在 Mapper 输出上反复运行。如果只有 1 或 2 个溢出文件，此时说明 Mapper 输出吞吐量降低，因而不值得调用 Combiner 带来额外开销。</p>
<p>由于 Combiner 与 Reducer 聚合逻辑相同，Hadoop 没有提供额外的 Combiner 类，而是通过 Reducer 类复用。新版的 CombinerRunner 代码如下，它通过反射获取了一个 Reducer 实例，直接运行 Reducer 的 <code>run()</code> 方法进行数据聚合，聚合结果由上下文对象传入 OutputCollector 中，最后写入磁盘。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NewCombinerRunner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">CombinerRunner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">combine</span><span class="params">(RawKeyValueIterator iterator, OutputCollector&lt;K, V&gt; collector)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        <span class="comment">// make a reducer</span></span><br><span class="line">        Reducer&lt;K, V, K, V&gt; reducer = (Reducer&lt;K, V, K, V&gt;) ReflectionUtils.newInstance(reducerClass, job);</span><br><span class="line">        Context reducerContext = createReduceContext(reducer, job, taskId,</span><br><span class="line">                iterator, <span class="keyword">null</span>, inputCounter,</span><br><span class="line">                <span class="keyword">new</span> OutputConverter(collector),</span><br><span class="line">                committer,</span><br><span class="line">                reporter, comparator, keyClass,</span><br><span class="line">                valueClass);</span><br><span class="line">        reducer.run(reducerContext);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>用户在声明作业配置时可以直接复用定义好的 Reducer 类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">job.setReducerClass(IntSumReducer.class);</span><br></pre></td></tr></table></figure>

<h3 id="Shuffler-阶段"><a href="#Shuffler-阶段" class="headerlink" title="Shuffler 阶段"></a>Shuffler 阶段</h3><p>数据混洗（Shuffle）被称为是 MapReduce 的“心脏”，是奇迹发生的地方。从宏观上，它体现为从 Map 任务端到 Reduce 任务端的数据流处理过程，具体说即是 <strong>Mapper 产生的中间键值对被重新组织（排序、分区），写入到本地磁盘中（溢写），Reducer 从多个 Mapper 工作节点上拷贝、合并数据</strong>的过程。最终目的是将数据从 Mapper 端发送到 Reducer 端，中间过程则是 MapReduce 框架基于性能考量的优化过程。从下图你可以清晰的体会到为什么这个过程被称为“数据混洗”。</p>
<p><img data-src="/2021/10/22/%E7%BB%93%E5%90%88%E6%BA%90%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-MapReduce-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/5.png" alt="2abd2dc93aa589f4b1d42b451b2b873c.png"></p>
<p>在微观上，Shuffle 被划分为了 Reduce 任务阶段的工作，定义在 <code>org.apache.hadoop.mapreduce.task.reduce</code> 包内。它是一个具体类，是框架定义的 <code>ShuffleConsumerPlugin&lt;K, V&gt;</code> 接口的内置实现类。从接口命名上可以体现出它作为 Mapper 端输出数据“消费者”的身份，“插件”则体现了它的可插拔特性，框架允许用户使用除了内置实现以外的三方插件。</p>
<p>在 ReduceTask 类的 <code>run()</code> 方法中通过反射获取了一个 Shuffle 类实例。<code>job.getClass()</code> 的第一个参数是三方插件的配置地址，第二个值 <code>Shuffle.class</code> 即为默认的内置实现类。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReduceTask</span> <span class="keyword">extends</span> <span class="title">Task</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(JobConf job, <span class="keyword">final</span> TaskUmbilicalProtocol umbilical)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">        ShuffleConsumerPlugin shuffleConsumerPlugin = <span class="keyword">null</span>;</span><br><span class="line">        Class&lt;? extends ShuffleConsumerPlugin&gt; clazz = job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);</span><br><span class="line">        shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);</span><br><span class="line"></span><br><span class="line">        shuffleConsumerPlugin.init(shuffleContext);</span><br><span class="line">        rIter = shuffleConsumerPlugin.run();</span><br><span class="line">        <span class="keyword">if</span> (useNewApi) &#123;</span><br><span class="line">            runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            runOldReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass);</span><br><span class="line">        &#125;</span><br><span class="line">        shuffleConsumerPlugin.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Shuffle 类的 <code>run()</code> 方法，是数据混洗步骤真正的具体实现。</p>
<h4 id="源码分析-1"><a href="#源码分析-1" class="headerlink" title="源码分析"></a>源码分析</h4><p>在一个 Map 任务完成后，会通过心跳包通知 JobTracker，这样，JobTracker 就能获取 Map 输出与主机位置之间的映射关系，Reduce 任务中的一个线程定期询问 JobTracker 以获取 Map 输出位置，直到获取所有输出位置。当一个 Map 任务完成后，Reduce 任务就可以开始复制了，这就是 <strong>Reduce 任务的复制（copy）阶段</strong>。此过程由并发线程完成，默认是 5 个线程，由 <code>mapreduce.reduce.shuffle.parallelcopies</code> 属性指定。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Shuffle</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">ShuffleConsumerPlugin</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;, <span class="title">ExceptionReporter</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RawKeyValueIterator <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">// Start the map-completion events fetcher thread</span></span><br><span class="line">        <span class="keyword">final</span> EventFetcher&lt;K, V&gt; eventFetcher = <span class="keyword">new</span> EventFetcher&lt;K, V&gt;(reduceId, umbilical, scheduler, <span class="keyword">this</span>, maxEventsToFetch);</span><br><span class="line">        eventFetcher.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Start the map-output fetcher threads</span></span><br><span class="line">        <span class="keyword">boolean</span> isLocal = localMapFiles != <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> numFetchers = isLocal ? <span class="number">1</span> : jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, <span class="number">5</span>);</span><br><span class="line">        Fetcher&lt;K, V&gt;[] fetchers = <span class="keyword">new</span> Fetcher[numFetchers];</span><br><span class="line">        <span class="keyword">if</span> (isLocal) &#123;</span><br><span class="line">            fetchers[<span class="number">0</span>] = <span class="keyword">new</span> LocalFetcher&lt;K, V&gt;(jobConf, reduceId, scheduler, merger, reporter, metrics, <span class="keyword">this</span>, reduceTask.getShuffleSecret(), localMapFiles);</span><br><span class="line">            fetchers[<span class="number">0</span>].start();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numFetchers; ++i) &#123;</span><br><span class="line">                fetchers[i] = <span class="keyword">new</span> Fetcher&lt;K, V&gt;(jobConf, reduceId, scheduler, merger, reporter, metrics, <span class="keyword">this</span>, reduceTask.getShuffleSecret());</span><br><span class="line">                fetchers[i].start();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Shuffle 使用名为 Fetcher 的线程类进行数据复制，如果 Reduce 任务恰好就处于运行 Map 任务的节点上，此时不需要网络通信，直接通过 LocalFetcher 获取本地数据。否则，使用 Fetcher 通过 HTTP 通信拉取相关节点上的数据。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fetcher</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// If merge is on, block</span></span><br><span class="line">        merger.waitForResource();</span><br><span class="line">        <span class="comment">// Get a host to shuffle from</span></span><br><span class="line">        host = scheduler.getHost();</span><br><span class="line">        metrics.threadBusy();</span><br><span class="line">        <span class="comment">// Shuffle</span></span><br><span class="line">        copyFromHost(host);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注：拉取后的数据是写入内存还是写入磁盘，是由 MapOutput 类的 <code>shuffle()</code> 方法完成的，该类拥有两个具体子类：InMemoryMapOutput 和 OnDiskMapOutput。</p>
<p>如果 Map 任务输出相当小，会被复制到 Reduce 任务的 JVM 内存缓冲区中，一旦内存缓冲区达到阈值，Map 输出会被复制到磁盘。随着磁盘上副本增多，后台线程会将它们合并为更大的、按键排好序的文件，这就是 Reduce 任务的<strong>合并阶段</strong>（merge）。这个阶段将合并 Map 输出，并维持其按键排序。合并的数据可能来自于内存缓冲区和磁盘文件。</p>
<p>合并阶段按照预先设定的合并因子（默认为 10），每趟合并 10 个文件，所以合并过程是循环进行的。为减少磁盘读写次数，最后一轮的文件合并（包含所有数据）不再写入磁盘，而是直接传送给 Reducer 处理。</p>
<h3 id="Reducer-阶段"><a href="#Reducer-阶段" class="headerlink" title="Reducer 阶段"></a>Reducer 阶段</h3><p>在 Reducer 阶段，对已排序的输出中的每个键调用 reduce 函数。此阶段的输出直接写到输出文件系统，一般为 HDFS，还可以是数据库。如果采用 HDFS，由于 Reduce 任务运行所在的节点也运行 DataNode，所以第一个块副本将被写到本地磁盘中。</p>
<p>用户需要继承 Reducer 并重写 reduce 函数，函数的输入值入参是值的可迭代对象，框架会从文件系统中读取输入键对应的值集合，传入给 reduce 函数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Reducer</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (VALUEIN value : values) &#123;</span><br><span class="line">            context.write((KEYOUT) key, (VALUEOUT) value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>与 Mapper 类似，框架也允许用户覆盖 <code>run()</code> 方法，以及 Context 对象的生命周期方法。</p>
<h3 id="数据写入阶段"><a href="#数据写入阶段" class="headerlink" title="数据写入阶段"></a>数据写入阶段</h3><p>数据写入阶段，由 OutputFormat 负责检验作业的输出规范，如输出目录是否已存在。如果写入到文件系统，则由实现子类 FileOutputformat 的 <code>setOutputPath()</code> 方法负责从作业配置中读取输出目录。由 OutputFormat 创建的 RecordWriter 对象负责将 Reducer 产生的键值对数据写到输出文件，每个 Reducer 对应一个文件。此过程无需创建分片。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">OutputFormat</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> RecordWriter&lt;K, V&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">checkOutputSpecs</span><span class="params">(JobContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> OutputCommitter <span class="title">getOutputCommitter</span><span class="params">(TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce-的局限"><a href="#MapReduce-的局限" class="headerlink" title="MapReduce 的局限"></a>MapReduce 的局限</h2><p>在 MapReduce 的工作过程中，框架需要频繁地读写文件系统，MapReduce 作业往往又都是数据密集型的，因此大量的中间数据会被往复地写入、读取、合并排序后又写入磁盘。大量的磁盘 I/O 导致 MapReduce 的耗时往往是分钟级、甚至是小时级的。受到 MapReduce 诞生年代的约束，昂贵的内存迫使用户将 Hadoop 集群部署在廉价的商用机器集群上，使用磁盘来进行数据的缓存。大规模的分布式部署使得 MapReduce 程序具有高容错性和良好的横向扩展性优势。</p>
<p>时至今日，用户对数据处理延迟的忍耐性越来越低，在大规模数据批处理之上又诞生了实时性要求极高的流处理系统。不管是 Spark Streaming 还是 Flink 都能做到秒级甚至是毫秒级的响应，MapReduce 因为其性能局限已经跟不上时代的需求。</p>
<p>MapReduce 的函数本身是无状态的，这意味着并不是所有工作 MapReduce 都能胜任，比如需要状态共享和参数依赖的机器学习模型训练算法。尽管 MapReduce 可以通过文件存储状态，但这样带来的性能开销是巨大的。相反，Spark 作为一个基于内存迭代式的大数据计算引擎很适合这样的场景，其提供了有状态的流来应对需要状态共享的作业，通过 <code>updateStateByKey()</code> 和 <code>mapWithState()</code> 状态管理函数共享状态。状态被保存在内存中，后续访问直接从内存中读取。据 Spark 官方统计的 Spark 运行逻辑回归机器学习算法的运行时间要优于 hadoop 一百倍。这也让 Spark 提供机器学习库 Spark MLlib 成为可能。</p>
<p>MapReduce 框架要求用户编写底层的 map 和 reduce 函数，这对于数据分析师是一个考验，Spark SQL 允许数据分析师使用 SQL 语言处理数据，由框架负责将其翻译成底层执行步骤。而且，编写底层函数这样的控制粒度不够灵活，对于一个复杂的作业来说，可能要由多个 MapReduce 作业组合而成，这样又会引入额外的中间数据读写开销。相比之下，Spark 编程模型更灵活，提供了丰富的 Transformation 和 Action 算子。性能方面，Spark 会为作业构建逻辑执行计划图（DAG），并针对其步骤进行优化，减少数据混洗阶段的 I/O 次数。</p>
<p>不管怎么说，MapReduce 作为第一代分布式计算引擎，其后诞生的分布式框架都是在其基础上的演进，因此有必要对 MapReduce 工作原理做个详细了解。Spark 由批处理起家，延续了 MapReduce 编程模型的设计思路。下一节我们将介绍 Spark 的运行原理。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>Hadoop 权威指南（第四版）</li>
<li><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Mapper">Apache Hadoop 官方文档：MapReduce Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://techvidvan.com/tutorials/how-mapreduce-works/">Phases of MapReduce – How Hadoop MapReduce Works</a></li>
<li><a target="_blank" rel="noopener" href="http://hadoopbeforestarting.blogspot.com/2012/12/difference-between-hadoop-old-api-and.html">Difference  between Hadoop OLD API and NEW API</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/" class="post-title-link" itemprop="url">使用 Docker 快速部署 Spark + Hadoop 大数据集群</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-12 21:47:58" itemprop="dateCreated datePublished" datetime="2021-10-12T21:47:58+08:00">2021-10-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-17 13:47:55" itemprop="dateModified" datetime="2021-12-17T13:47:55+08:00">2021-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Ops/" itemprop="url" rel="index"><span itemprop="name">Ops</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>为了免去繁杂的环境配置工作，提供开箱即用的 Spark + Hadoop 快捷部署方案。本教程基于 BitNami 项目的成熟镜像方案，搭建 Spark Docker 集群，并在原有镜像基础上，构建了安装有对应版本 Hadoop 的镜像。</p>
<p>镜像已提交至 Docker Hub 官方仓库中，可通过如下命令拉取：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull s1mplecc/spark-hadoop:3</span><br></pre></td></tr></table></figure>

<p>构建镜像的所需文件也已提交至 GitHub：<a target="_blank" rel="noopener" href="https://github.com/s1mplecc/spark-hadoop-docker">s1mplecc/spark-hadoop-docker</a>。</p>
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><ul>
<li>操作系统 MacOS Mojave，命令行工具：Terminal + Zsh</li>
<li>Docker Desktop for Mac，内置 Docker CLI client 与 Docker Compose</li>
<li>Spark Docker 镜像：<a target="_blank" rel="noopener" href="https://github.com/bitnami/bitnami-docker-spark">bitnami-docker-spark</a>，Spark 版本：3.1.2</li>
<li>Hadoop 版本：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/release/3.2.0.html">hadoop-3.2.0</a></li>
</ul>
<h2 id="部署-Spark-集群"><a href="#部署-Spark-集群" class="headerlink" title="部署 Spark 集群"></a>部署 Spark 集群</h2><h3 id="拉取镜像"><a href="#拉取镜像" class="headerlink" title="拉取镜像"></a>拉取镜像</h3><p><a target="_blank" rel="noopener" href="https://github.com/bitnami">BitNami</a> 是一个开源项目，现已被 VMware 公司收购，其宗旨是简化在个人终端、Kubernetes 和云服务器等环境上的开源软件的部署。其已为 Docker Hub 社区提供了数百个容器镜像方案，其中的 Redis、MongoDB 等热门镜像更是超过了十亿次下载。</p>
<p>bitnami/spark 镜像也已超过百万次下载，这是一个成熟的 Spark Docker 方案。此外选择它的重要原因是它的文档齐全，且更新频率快，目前的最新版本基于 Spark 官方发行的最新版本 Spark 3.1.2。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ docker pull bitnami/spark:3</span><br><span class="line">➜ docker images       </span><br><span class="line">REPOSITORY               TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">bitnami/spark            3         67ed3ae333e0   4 days ago      1.29GB</span><br></pre></td></tr></table></figure>

<p>注：此镜像基于 bitnami/minideb 基础镜像，这是 BitNami 构建的极简 Debian 系统镜像。Debian 由于系统稳定且内核占用资源小的优势，非常适合作为服务器操作系统。</p>
<h3 id="以集群方式运行"><a href="#以集群方式运行" class="headerlink" title="以集群方式运行"></a>以集群方式运行</h3><p>为了模拟 Spark 集群，采取一主二从的部署方式，使用 Docker Compose 对容器集群进行统一编排管理。</p>
<p>首先，在本地新建一个工作目录，我的路径为 <code>~/docker/spark</code>，在该目录下编写 docker-compose.yml 配置文件。基于 bitnami/spark 提供的配置文件，我做了一些修改，包括：</p>
<ul>
<li><code>hostname</code>：容器实例主机名；</li>
<li><code>volumes</code>：挂载本地目录 <code>~/docker/spark/share</code> 到容器目录 <code>/opt/share</code>；</li>
<li><code>ports</code>：开放 4040 和从节点 Spark Web UI 端口。</li>
</ul>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">spark:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">docker.io/bitnami/spark:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">master</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=master</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8080:8080&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;4040:4040&#x27;</span></span><br><span class="line">  <span class="attr">spark-worker-1:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">docker.io/bitnami/spark:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">worker1</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=worker</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MASTER_URL=spark://master:7077</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_MEMORY=1G</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_CORES=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8081:8081&#x27;</span></span><br><span class="line">  <span class="attr">spark-worker-2:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">docker.io/bitnami/spark:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">worker2</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=worker</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MASTER_URL=spark://master:7077</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_MEMORY=1G</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_CORES=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8082:8081&#x27;</span></span><br></pre></td></tr></table></figure>

<p>现在可以启动 Spark Docker 集群了。在工作目录下，执行如下命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ docker-compose up -d                        </span><br><span class="line">[+] Running 3/3</span><br><span class="line"> ⠿ Container spark-spark-1           Started                                         1.0s</span><br><span class="line"> ⠿ Container spark-spark-worker-2-1  Started                                         1.1s</span><br><span class="line"> ⠿ Container spark-spark-worker-1-1  Started                                         1.1s</span><br></pre></td></tr></table></figure>

<p>启动后的集群可以在 Docker Desktop 中进行查看：</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/1.png" alt="7262ff48300734f66dd1a10cce5a064e.png"></p>
<p>可通过映射的端口访问 Spark Web UI。集群以默认的 Standalone 独立集群模式启动，通过 <a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080/</a> 查看集群运行状态：</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/2.png" alt="556c8dffa7f55d32d78b52c11fe31701.png"></p>
<h4 id="集群网络"><a href="#集群网络" class="headerlink" title="集群网络"></a>集群网络</h4><p>默认情况下，通过 <code>docker-compose</code> 启动的容器集群，会创建并使用名为 <code>镜像名_default</code> 的桥接网络，如 <code>spark_default</code>。集群内的容器处于同一子网网段，因此可以相互通信。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ docker network ls         </span><br><span class="line">NETWORK ID     NAME            DRIVER    SCOPE</span><br><span class="line">331df1b4ff6d   bridge          bridge    local</span><br><span class="line">3a916c4f1299   host            host      local</span><br><span class="line">42b893852f97   none            null      local</span><br><span class="line">e425e615144b   spark_default   bridge    local</span><br></pre></td></tr></table></figure>

<p>通过 <code>inspect</code> 命令查看网络配置详情。以下是 <code>spark_default</code> 网络部分配置信息，其使用 <code>172.18.0.0/16</code> 子网网段，并为每个容器实例分配了 IPv4 地址。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">➜ docker network inspect e4</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="attr">&quot;Name&quot;</span>: <span class="string">&quot;spark_default&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;Id&quot;</span>: <span class="string">&quot;e425e615144b972265afa9c6e78d7bf22cab446bc5bdece3f188abf5879e8677&quot;</span>,   # NETWORK ID</span><br><span class="line">        <span class="attr">&quot;IPAM&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;Config&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="attr">&quot;Subnet&quot;</span>: <span class="string">&quot;172.18.0.0/16&quot;</span>,</span><br><span class="line">                    <span class="attr">&quot;Gateway&quot;</span>: <span class="string">&quot;172.18.0.1&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;Containers&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;9b9d9c1bb6f873da6b3c72232afab3a451b12cdd4c19d51cc510de1854f95efd&quot;</span>: &#123;   # CONTAINER ID</span><br><span class="line">                <span class="attr">&quot;Name&quot;</span>: <span class="string">&quot;spark-spark-worker-1-1&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.2/16&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;a162bd33dc1936c70259ed2146a1f4a30e52faf66a91624d794336b5357a5f7b&quot;</span>: &#123;</span><br><span class="line">                <span class="attr">&quot;Name&quot;</span>: <span class="string">&quot;spark-spark-worker-2-1&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.4/16&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="attr">&quot;b6ad535e924221e30746722cd486dace692f0f42528eba57347ef4177b355855&quot;</span>: &#123;</span><br><span class="line">                <span class="attr">&quot;Name&quot;</span>: <span class="string">&quot;spark-spark-1&quot;</span>,</span><br><span class="line">                <span class="attr">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.3/16&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>注：同 CONTAINER ID 一样，NETWORK ID 的前两位可以唯一标识一个网络，如 <code>e4</code>。</p>
<h3 id="使用-Spark-Shell-进行交互"><a href="#使用-Spark-Shell-进行交互" class="headerlink" title="使用 Spark Shell 进行交互"></a>使用 Spark Shell 进行交互</h3><p>查看正在运行的容器实例，找到 master 实例的容器 ID：a162bd33dc19。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ docker ps                </span><br><span class="line">CONTAINER ID   IMAGE                     COMMAND                  CREATED        STATUS             PORTS                                                                                                                                              NAMES</span><br><span class="line">a162bd33dc19   s1mplecc/spark-hadoop:3   <span class="string">&quot;/opt/bitnami/script…&quot;</span>   42 hours ago   Up About an hour   0.0.0.0:4040-&gt;4040/tcp, 0.0.0.0:8080-&gt;8080/tcp   spark-spark-1</span><br><span class="line">9b9d9c1bb6f8   s1mplecc/spark-hadoop:3   <span class="string">&quot;/opt/bitnami/script…&quot;</span>   42 hours ago   Up About an hour   0.0.0.0:8081-&gt;8081/tcp                                                                                                                             spark-spark-worker-1-1</span><br><span class="line">b6ad535e9242   s1mplecc/spark-hadoop:3   <span class="string">&quot;/opt/bitnami/script…&quot;</span>   42 hours ago   Up About an hour   0.0.0.0:8082-&gt;8081/tcp                                                                                                                             spark-spark-worker-2-1</span><br></pre></td></tr></table></figure>

<p>执行如下命令进入到 master 容器内部。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ docker <span class="built_in">exec</span> -it a1 bash</span><br><span class="line">I have no name!@master:/opt/bitnami/spark$ </span><br></pre></td></tr></table></figure>

<p>注：实际上 <code>-it</code> 参数的作用是分配一个交互式虚拟终端；容器 ID 的前两位可以唯一标识该容器，如 a1。</p>
<p>现在，可以通过 <code>pyspark</code> 或 <code>spark-shell</code> 命令启动 Spark 交互式命令行，下面以 <code>pyspark</code> 为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ pyspark </span><br><span class="line">Python 3.6.15 (default, Sep 24 2021, 11:37:20) </span><br><span class="line">[GCC 8.3.0] on linux</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.6.15 (default, Sep 24 2021 11:37:20)</span><br><span class="line">Spark context Web UI available at http://master:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1633913993830).</span><br><span class="line">SparkSession available as &#x27;spark&#x27;.</span><br></pre></td></tr></table></figure>

<p>在启动交互式 Shell 时，<strong>Spark 驱动器程序（Driver Program）会创建一个名为 sc 的 SparkContext 对象，我们可以通过该对象来创建 RDD</strong>。例如，通过 <code>sc.textFile()</code> 方法读取本地或 HDFS 文件，或者通过 <code>sc.parallelize()</code> 方法直接由 Python 集合创建 RDD。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines = sc.textFile(<span class="string">&#x27;README.md&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines.count()</span><br><span class="line"><span class="number">108</span>                                   </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines.<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: <span class="built_in">len</span>(line) &gt; <span class="number">10</span>)</span><br><span class="line">PythonRDD[<span class="number">3</span>] at RDD at PythonRDD.scala:<span class="number">53</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines.<span class="built_in">filter</span>(<span class="keyword">lambda</span> line: <span class="built_in">len</span>(line) &gt; <span class="number">10</span>).count()</span><br><span class="line"><span class="number">67</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>strs = sc.parallelize([<span class="string">&#x27;hello world&#x27;</span>, <span class="string">&#x27;i am spark&#x27;</span>, <span class="string">&#x27;hadoop&#x27;</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>strs.flatMap(<span class="keyword">lambda</span> s: s.split(<span class="string">&#x27; &#x27;</span>)).collect()</span><br><span class="line">[<span class="string">&#x27;hello&#x27;</span>, <span class="string">&#x27;world&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;am&#x27;</span>, <span class="string">&#x27;spark&#x27;</span>, <span class="string">&#x27;hadoop&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>strs.flatMap(<span class="keyword">lambda</span> s: s.split(<span class="string">&#x27; &#x27;</span>)).reduce(<span class="keyword">lambda</span> x, y: x + <span class="string">&#x27;-&#x27;</span> + y)</span><br><span class="line"><span class="string">&#x27;hello-world-i-am-spark-hadoop&#x27;</span></span><br></pre></td></tr></table></figure>

<p>注：由于 Spark 的<strong>惰性求值</strong>特性，只有当执行 Action 操作时，如 count、collect、reduce，才会真正执行计算并返回结果。</p>
<p>Spark Shell 默认以本地模式运行，但也支持以集群模式运行。可以通过指定 <code>--master</code> 参数，如 <code>pyspark --master spark://master:7077</code>，以 Standalone 模式运行 PySpark Shell。</p>
<h3 id="使用-spark-submit-提交独立应用"><a href="#使用-spark-submit-提交独立应用" class="headerlink" title="使用 spark-submit 提交独立应用"></a>使用 spark-submit 提交独立应用</h3><p>Spark Shell 支持与存储在硬盘或内存上的分布式数据进行交互，如 HDFS。因此 Spark Shell 适用于即时数据分析，比如数据探索阶段。但我们的最终目的是创建一个独立的 Java、Scala 或 Python 应用，将其提交到 Spark 集群上运行。</p>
<p>Spark 为各种集群管理器提供了统一的工具来提交作业，这个工具就是 <code>spark-submit</code>。</p>
<h4 id="提交-Python-应用"><a href="#提交-Python-应用" class="headerlink" title="提交 Python 应用"></a>提交 Python 应用</h4><p>下面以 Python 应用为例，编写 Python 脚本 my_script.py。Python 的 Spark 依赖库就叫做 pyspark，已经包含在 Spark 安装包内，位于 <code>$SPARK_HOME/python</code> 目录下。在独立应用中，我们需要导入该依赖，并且手动创建一个 SparkContext 实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line">conf = SparkConf().setAppName(<span class="string">&#x27;My App&#x27;</span>)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">count = sc.<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">1000</span> * <span class="number">1000</span> * <span class="number">100</span>).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x &gt; <span class="number">100</span>).count()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;count: &#x27;</span>, count)</span><br></pre></td></tr></table></figure>

<p>注：SparkConf 用于声明应用配置信息，可以编码在代码中，也可以在命令行以参数形式指定。SparkConf 读取优先级为：代码 &gt; <code>spark-submit</code> 命令行参数 &gt; 以 <code>--properties-file</code> 参数指定的配置文件 &gt; 系统默认配置。</p>
<p>使用 <code>spark-submit</code> 命令提交 Python 脚本，指定 <code>--master</code> 参数提交到集群上。如果未指定该参数，则默认以本地模式运行。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://master:7077 /opt/share/my_script.py</span><br><span class="line">count:  99999899</span><br></pre></td></tr></table></figure>

<p>当指定为 <code>spark://host:port</code> 时，应用提交到 Spark 自带的独立集群管理器（Standalone）上运行，默认端口 7077；如果使用 Apache Mesos 集群管理器，需指定为 <code>mesos://host:port</code>，默认端口 5050；使用 Hadoop YARN 则需指定为 <code>yarn</code>。</p>
<p>应用运行时，SparkContext 实例会启动应用的 Web UI，默认端口 4040。你可以在此网址查看应用的作业（Job）、组成作业的所有步骤（Stage）、持久化的 RDD 以及执行器状态等信息，这对于应用性能评估有巨大帮助。运行结束后，SparkContext 实例消亡，同时会关闭此 Web UI。</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/3.png" alt="2626b1a665dbf00b5641f10100befa4e.png"></p>
<h4 id="提交-Java-应用"><a href="#提交-Java-应用" class="headerlink" title="提交 Java 应用"></a>提交 Java 应用</h4><p>Spark 安装包内置了可以运行的示例 Jar 包，位于 <code>$SPARK_HOME/examples</code> 目录下。向 Spark 集群提交 Jar 包需额外指定程序入口类，即 main 函数所在类。可以通过 <code>jar tf</code> 命令查看 Jar 包所包含的类。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ jar tf /opt/bitnami/spark/examples/jars/spark-examples_2.12-3.1.2.jar | grep WordCount</span><br><span class="line">org/apache/spark/examples/JavaWordCount.class</span><br><span class="line">org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.class</span><br><span class="line">org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.class</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>找到统计词频的入口类 <code>org.apache.spark.examples.JavaWordCount</code>，以此为例，向 Spark 集群提交 Java 应用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master spark://master:7077 \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--name &quot;JavaWordCount&quot; \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--class org.apache.spark.examples.JavaWordCount \</span><br><span class="line">/opt/bitnami/spark/examples/jars/spark-examples_2.12-3.1.2.jar /opt/share/words.txt</span><br></pre></td></tr></table></figure>

<p>注：<code>--deploy-mode</code> 参数决定了驱动器程序的运行位置。默认情况下，即客户端模式（client）下，<code>spark-submit</code> 会在本地（运行该命令的机器上）启动驱动器程序。如果指定为集群模式（cluster），驱动器程序将会运行在随机选取的一个工作节点上，此时即使 ctrl-c 中断 <code>spark-submit</code> 命令，也不会影响应用继续运行。因此，集群模式适用于需要长时间作业的应用。此外，Spark Standalone 目前不支持以集群模式运行 Python 应用（可以使用 YARN 集群来解决）。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/3.1.2/submitting-applications.html">Spark 3.1.2 官方文档</a>: Currently, the standalone mode does not support cluster mode for Python applications.</p>
</blockquote>
<p>当通过集群模式运行上述命令时，驱动器程序 Driver 并不是运行主节点上，而是运行在 IP 为 <code>172.18.0.2</code> 的工作节点上：</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/4.png" alt="9dbf3ac21791155581e6c3e71924002c.png"></p>
<h2 id="安装-Hadoop"><a href="#安装-Hadoop" class="headerlink" title="安装 Hadoop"></a>安装 Hadoop</h2><p>Hadoop 由分布式文件系统 HDFS、分布式计算框架 MapReduce 和资源管理框架 YARN 组成。MapReduce 是面向磁盘的，运行效率受到磁盘读写性能的约束，Spark 延续了 MapReduce 编程模型的设计思路，提出了面向内存的分布式计算框架，性能较之 MapReduce 有了 10～100 倍的提升。与此同时，Spark 框架还对 HDFS 做了很好的支持，并支持运行在 YARN 集群上。</p>
<blockquote>
<p>Spark uses Hadoop’s client libraries for HDFS and YARN. Downloads are pre-packaged for a handful of popular Hadoop versions.</p>
</blockquote>
<p>由于 Spark 使用了 Hadoop 的客户端依赖库，所以 Spark 安装包会指定依赖的 Hadoop 特定版本，如 spark-3.1.2-bin-hadoop3.2.tgz。而 bitnami/spark 镜像中只包含 Hadoop 客户端，并不包含服务器端。因此，如果需要使用 HDFS 和 YARN 功能，还需要部署 Hadoop 集群。</p>
<p>将 Hadoop 部署在 Spark 集群上，可以避免不必要的网络通信，并且面向磁盘的 HDFS 与面向内存的 Spark 天生互补。因此，考虑在 bitnami/spark 镜像基础上构建安装有 Hadoop 的新镜像。</p>
<h3 id="确定-Hadoop-版本"><a href="#确定-Hadoop-版本" class="headerlink" title="确定 Hadoop 版本"></a>确定 Hadoop 版本</h3><p>首先，需要确定 bitnami/spark 镜像所依赖的 Hadoop 版本。启动 pyspark 进行查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pyspark</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc._gateway.jvm.org.apache.hadoop.util.VersionInfo.getVersion()</span><br><span class="line"><span class="string">&#x27;3.2.0&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在 Hadoop 官网找到 Hadoop 3.2.0 安装包的<a target="_blank" rel="noopener" href="https://hadoop.apache.org/release/3.2.0.html">下载地址</a>，稍后在构建镜像时通过 <code>curl -OL</code> 命令下载此安装包。</p>
<h3 id="准备配置文件及启动脚本"><a href="#准备配置文件及启动脚本" class="headerlink" title="准备配置文件及启动脚本"></a>准备配置文件及启动脚本</h3><p>在工作目录下创建 config 文件夹，编写需要覆盖的 Hadoop 配置文件。完整的配置文件已经上传至 GitHub：<a target="_blank" rel="noopener" href="https://github.com/s1mplecc/spark-hadoop-docker">s1mplecc/spark-hadoop-docker</a>。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ tree ~/docker/spark/config</span><br><span class="line">config</span><br><span class="line">├── core-site.xml</span><br><span class="line">├── hadoop-env.sh</span><br><span class="line">├── hdfs-site.xml</span><br><span class="line">├── mapred-site.xml</span><br><span class="line">├── workers</span><br><span class="line">└── yarn-site.xml</span><br></pre></td></tr></table></figure>

<p>注：其他详细配置请参考 Apache Hadoop 官方文档：<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop Cluster Setup</a>。</p>
<p>除了配置文件外，还需要编写 Hadoop 启动脚本。由于设置了 ssh 免密通信，首先需要启动 ssh 服务，然后依次启动 HDFS 和 YARN 集群。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">service ssh start</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh</span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="基于-bitnami-spark-构建新镜像"><a href="#基于-bitnami-spark-构建新镜像" class="headerlink" title="基于 bitnami/spark 构建新镜像"></a>基于 bitnami/spark 构建新镜像</h3><p>在工作目录下，创建用于构建新镜像的 Dockerfile。新镜像基于 <code>docker.io/bitnami/spark:3</code>，依次执行如下指令：</p>
<ul>
<li>设置 Hadoop 环境变量；</li>
<li>配置集群间 ssh 免密通信。此处直接将 ssh-keygen 工具生成的公钥写入 authorized_keys 文件中，由于容器集群基于同一个镜像创建的，因此集群的公钥都相同且 authorized_keys 为自己本身；</li>
<li>下载 Hadoop 3.2.0 安装包并解压；</li>
<li>创建 HDFS NameNode 和 DataNode 工作目录；</li>
<li>覆盖 <code>$HADOOP_CONF_DIR</code> 目录下的 Hadoop 配置文件；</li>
<li>拷贝 Hadoop 启动脚本并设置为可执行文件；</li>
<li>格式化 HDFS 文件系统。</li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> docker.io/bitnami/spark:<span class="number">3</span></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">&quot;s1mplecc &lt;s1mple951205@gmail.com&gt;&quot;</span></span></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> description=<span class="string">&quot;Docker image with Spark (3.1.2) and Hadoop (3.2.0), based on bitnami/spark:3. \</span></span></span><br><span class="line"><span class="string"><span class="bash">For more information, please visit https://github.com/s1mplecc/spark-hadoop-docker.&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> HADOOP_HOME=<span class="string">&quot;/opt/hadoop&quot;</span></span><br><span class="line"><span class="keyword">ENV</span> HADOOP_CONF_DIR=<span class="string">&quot;$HADOOP_HOME/etc/hadoop&quot;</span></span><br><span class="line"><span class="keyword">ENV</span> HADOOP_LOG_DIR=<span class="string">&quot;/var/log/hadoop&quot;</span></span><br><span class="line"><span class="keyword">ENV</span> PATH=<span class="string">&quot;$HADOOP_HOME/hadoop/sbin:$HADOOP_HOME/bin:$PATH&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /opt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update &amp;&amp; apt-get install -y openssh-server</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> ssh-keygen -t rsa -f /root/.ssh/id_rsa -P <span class="string">&#x27;&#x27;</span> &amp;&amp; \</span></span><br><span class="line"><span class="bash">    cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> curl -OL https://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> tar -xzvf hadoop-3.2.0.tar.gz &amp;&amp; \</span></span><br><span class="line"><span class="bash">  mv hadoop-3.2.0 hadoop &amp;&amp; \</span></span><br><span class="line"><span class="bash">  rm -rf hadoop-3.2.0.tar.gz &amp;&amp; \</span></span><br><span class="line"><span class="bash">  mkdir /var/<span class="built_in">log</span>/hadoop</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir -p /root/hdfs/namenode &amp;&amp; \ </span></span><br><span class="line">    mkdir -p /root/hdfs/datanode </span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> config/* /tmp/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mv /tmp/ssh_config /root/.ssh/config &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mv /tmp/hadoop-env.sh <span class="variable">$HADOOP_CONF_DIR</span>/hadoop-env.sh &amp;&amp; \</span></span><br><span class="line"><span class="bash">    mv /tmp/hdfs-site.xml <span class="variable">$HADOOP_CONF_DIR</span>/hdfs-site.xml &amp;&amp; \ </span></span><br><span class="line">    mv /tmp/core-site.xml $HADOOP_CONF_DIR/core-site.xml &amp;&amp; \</span><br><span class="line">    mv /tmp/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml &amp;&amp; \</span><br><span class="line">    mv /tmp/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml &amp;&amp; \</span><br><span class="line">    mv /tmp/workers $HADOOP_CONF_DIR/workers</span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> start-hadoop.sh /opt/start-hadoop.sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chmod +x /opt/start-hadoop.sh &amp;&amp; \</span></span><br><span class="line"><span class="bash">    chmod +x <span class="variable">$HADOOP_HOME</span>/sbin/start-dfs.sh &amp;&amp; \</span></span><br><span class="line"><span class="bash">    chmod +x <span class="variable">$HADOOP_HOME</span>/sbin/start-yarn.sh </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> hdfs namenode -format</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [ <span class="string">&quot;/opt/bitnami/scripts/spark/entrypoint.sh&quot;</span> ]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [ <span class="string">&quot;/opt/bitnami/scripts/spark/run.sh&quot;</span> ]</span></span><br></pre></td></tr></table></figure>

<p>在工作目录下，执行如下命令构建镜像：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ docker build -t s1mplecc/spark-hadoop:3 .</span><br></pre></td></tr></table></figure>

<p>构建过程将按照 Dockerfile 中的指令依次进行。</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/5.png" alt="fcf37dc09ce643cfbdda2b473ac6a194.png"></p>
<h3 id="启动-spark-hadoop-集群"><a href="#启动-spark-hadoop-集群" class="headerlink" title="启动 spark-hadoop 集群"></a>启动 spark-hadoop 集群</h3><p>构建镜像完成后，还需要修改 <code>docker-compose.yml</code> 文件，使其从新的镜像 <code>s1mplecc/spark-hadoop:3</code> 中启动容器集群，同时映射 Hadoop Web UI 端口。</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">spark:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">s1mplecc/spark-hadoop:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">master</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=master</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8080:8080&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;4040:4040&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8088:8088&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8042:8042&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;9870:9870&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;19888:19888&#x27;</span></span><br><span class="line">  <span class="attr">spark-worker-1:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">s1mplecc/spark-hadoop:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">worker1</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=worker</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MASTER_URL=spark://master:7077</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_MEMORY=1G</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_CORES=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8081:8081&#x27;</span></span><br><span class="line">  <span class="attr">spark-worker-2:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">s1mplecc/spark-hadoop:3</span></span><br><span class="line">    <span class="attr">hostname:</span> <span class="string">worker2</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MODE=worker</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_MASTER_URL=spark://master:7077</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_MEMORY=1G</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_WORKER_CORES=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_AUTHENTICATION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_RPC_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">SPARK_SSL_ENABLED=no</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">~/docker/spark/share:/opt/share</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;8082:8081&#x27;</span></span><br></pre></td></tr></table></figure>

<p>运行 <code>docker-compose</code> 启动命令重建集群，不需要停止或删除旧集群。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ docker-compose up -d                     </span><br><span class="line">[+] Running 0/3</span><br><span class="line"> ⠼ Container spark-spark-1           Recreate                                        6.4s</span><br><span class="line"> ⠼ Container spark-spark-worker-2-1  Recreate                                        6.4s</span><br><span class="line"> ⠼ Container spark-spark-worker-1-1  Recreate                                        6.4s</span><br></pre></td></tr></table></figure>

<p>启动容器集群后，进入 master 容器执行启动脚本：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ docker <span class="built_in">exec</span> -it a1 bash</span><br><span class="line">$ ./start-hadoop.sh </span><br><span class="line">Starting OpenBSD Secure Shell server: sshd.</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">Starting secondary namenodes [master]</span><br><span class="line">Starting resourcemanager</span><br><span class="line">Starting nodemanagers</span><br></pre></td></tr></table></figure>

<h3 id="向-HDFS-写入文件"><a href="#向-HDFS-写入文件" class="headerlink" title="向 HDFS 写入文件"></a>向 HDFS 写入文件</h3><p>使用命令将共享文件中的 words.txt 写入 HDFS：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put /opt/share/words.txt /</span><br><span class="line">$ hdfs dfs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   2 root supergroup        440 2021-10-12 07:07 /words.txt</span><br></pre></td></tr></table></figure>

<p>写入的文件可以在 HDFS Web UI 上进行浏览：</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/6.png" alt="48ba5d8dbcb460ec421762e24565dd47.png"></p>
<h3 id="Spark-访问-HDFS"><a href="#Spark-访问-HDFS" class="headerlink" title="Spark 访问 HDFS"></a>Spark 访问 HDFS</h3><p>现在，可以通过 Spark 访问 HDFS 了，访问 URI 为 <code>hdfs://master:9000</code>，这是配置在 core-site.xml 文件中的默认文件系统 fs.defaultFS。下面通过 PySpark 演示如何读取和存储 HDFS 上的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ pyspark</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines = sc.textFile(<span class="string">&#x27;hdfs://master:9000/words.txt&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lines.collect()</span><br><span class="line">[<span class="string">&#x27;Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>words = lines.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>words.saveAsTextFile(<span class="string">&#x27;hdfs://master:9000/split-words.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>HDFS 上的文件被读取为 RDD，在内存上进行 Transformation 后写入 HDFS。写入的文件被存储到 HDFS 的 DataNode 块分区上。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs dfs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2021-10-12 13:28 /split-words.txt</span><br><span class="line">-rw-r--r--   2 root supergroup        440 2021-10-12 13:24 /words.txt</span><br><span class="line">$ hdfs dfs -ls /split-words.txt</span><br><span class="line">Found 3 items</span><br><span class="line">-rw-r--r--   2 root supergroup          0 2021-10-12 13:28 /split-words.txt/_SUCCESS</span><br><span class="line">-rw-r--r--   2 root supergroup        441 2021-10-12 13:28 /split-words.txt/part-00000</span><br><span class="line">-rw-r--r--   2 root supergroup          0 2021-10-12 13:28 /split-words.txt/part-00001</span><br><span class="line">$ hdfs dfs -cat /split-words.txt/part-00000</span><br><span class="line">Apache</span><br><span class="line">Spark</span><br><span class="line">is</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="将-Spark-应用提交到-YARN-集群"><a href="#将-Spark-应用提交到-YARN-集群" class="headerlink" title="将 Spark 应用提交到 YARN 集群"></a>将 Spark 应用提交到 YARN 集群</h3><p>在运行 Hadoop 启动脚本时同时启动了 HDFS 和 YARN，现在可以将 Spark 应用提交到 YARN 集群上。默认使用 HDFS 文件系统，如需读取本地文件，还需要指定 <code>file://</code> 前缀。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ spark-submit --master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--name <span class="string">&quot;Word Count&quot;</span> \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--class org.apache.spark.examples.JavaWordCount \</span><br><span class="line">/opt/bitnami/spark/examples/jars/spark-examples_2.12-3.1.2.jar /words.txt</span><br></pre></td></tr></table></figure>

<p>提交到 YARN 上的应用通过 ResourceManager Web UI 进行查看，默认端口 8088。</p>
<p><img data-src="/2021/10/12/%E4%BD%BF%E7%94%A8-Docker-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2-Spark-Hadoop-%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4/7.png" alt="21f342e77ffb7f24b02ffced7a931548.png"></p>
<h2 id="Web-UI-汇总"><a href="#Web-UI-汇总" class="headerlink" title="Web UI 汇总"></a>Web UI 汇总</h2><table>
<thead>
<tr>
<th align="center">Web UI</th>
<th align="center">默认网址</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center">* <strong>Spark Application</strong></td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:4040/">http://localhost:4040</a></td>
<td align="center">由 SparkContext 启动，显示以本地或 Standalone 模式运行的 Spark 应用</td>
</tr>
<tr>
<td align="center">Spark Standalone Master</td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a></td>
<td align="center">显示集群状态，以及以 Standalone 模式提交的 Spark 应用</td>
</tr>
<tr>
<td align="center">* <strong>HDFS NameNode</strong></td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:9870/">http://localhost:9870</a></td>
<td align="center">可浏览 HDFS 文件系统</td>
</tr>
<tr>
<td align="center">* <strong>YARN ResourceManager</strong></td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:8088/">http://localhost:8088</a></td>
<td align="center">显示提交到 YARN 上的 Spark 应用</td>
</tr>
<tr>
<td align="center">YARN NodeManager</td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:8042/">http://localhost:8042</a></td>
<td align="center">显示工作节点配置信息和运行时日志</td>
</tr>
<tr>
<td align="center">MapReduce Job History</td>
<td align="center"><a target="_blank" rel="noopener" href="http://localhost:19888/">http://localhost:19888</a></td>
<td align="center">MapReduce 历史任务</td>
</tr>
</tbody></table>
<p>注：星号标注的为较常用的 Web UI。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-web-ui-understanding/">Spark Web UI – Understanding Spark Execution</a></li>
<li><a target="_blank" rel="noopener" href="https://sparkbyexamples.com/spark/spark-deploy-modes-client-vs-cluster/">Spark Deploy Modes – Client vs Cluster Explained</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/networking/">Docker 官方文档：Networking in Compose</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">Docker 官方文档：Dockerfile reference</a></li>
<li><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.2.2/hadoop-project-dist/hadoop-common/ClusterSetup.html#Web_Interfaces">Apache Hadoop 官方文档：Hadoop Cluster Setup</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://s1mplecc.github.io/2021/05/12/Python-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="s1mple">
      <meta itemprop="description" content="春光恰与少年同，十里清风慕天青">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="芥子屋">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/12/Python-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/" class="post-title-link" itemprop="url">Python 面向对象</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-12 10:51:28" itemprop="dateCreated datePublished" datetime="2021-05-12T10:51:28+08:00">2021-05-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-17 13:47:55" itemprop="dateModified" datetime="2021-12-17T13:47:55+08:00">2021-12-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Languages/" itemprop="url" rel="index"><span itemprop="name">Languages</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h2><p><strong>面向对象程序设计</strong>（Object-oriented programming，缩写 OOP）是指一种程序设计范型，它强调一切行为都是基于<strong>对象</strong>（object）完成的，而对象则指的是<strong>类</strong>（class）的实例。对象被作为程序的基本单元，数据和行为方法封装在其中，以提高软件的重用性、灵活性和扩展性，对象的行为方法可以访问和修改对象的数据。通过对象之间的相互协作，完成复杂的程序功能。面向对象编程语言具备封装、抽象、继承、多态等特性。</p>
<p>封装，又称<strong>信息隐藏</strong>，是指利用抽象数据类型（ADT）将数据和基于数据的操作封装在一起，尽可能地隐藏内部细节，只暴露一些公共接口与外部发生交互。面向对象编程语言使用类进行封装，数据和基于数据的操作对应于类的属性和方法。</p>
<p>具备封装性的面向对象程序设计隐藏了方法的具体执行步骤，取而代之的是对象之间的消息传递。举个例子，假设一个“歌唱家”想要“唱歌”，她当然知道自己该如何发声，但其他人没有必要了解她发声的细节，只管欣赏她美妙的歌喉。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/* 一个面向过程的程序会这样写： */</span><br><span class="line">定义莱丝</span><br><span class="line">莱丝.设置音调(5)</span><br><span class="line">莱丝.吸气()</span><br><span class="line">莱丝.吐气()</span><br><span class="line"></span><br><span class="line">/* 当唱歌方法被封装到类中，任何歌唱家都可以简单地使用： */</span><br><span class="line">定义歌唱家类</span><br><span class="line">声明莱丝是一个歌唱家</span><br><span class="line">莱丝.唱歌()</span><br></pre></td></tr></table></figure>

<h3 id="访问限制"><a href="#访问限制" class="headerlink" title="访问限制"></a>访问限制</h3><p>使用封装能够对成员属性和方法进行精确的访问控制，通常来说，成员会依照它们的访问权限被分为3种：公有成员、私有成员以及保护成员，保护成员是指可以被子类访问的成员。有的语言更进一步：Java 专门提供了 public、private、protected 和缺省四个级别的访问权限控制关键字。Python 则更提倡开放，尽管没有强制要求，但也建议程序员使用带有下划线的命名风格来规范属性和方法的访问权限。</p>
<p>在 Python 中，非下划线开头的属性称为公有属性，单下划线或双下划线开头的属性称为私有属性，双下划线开头的私有属性不会被子类可见，Python 社区很少提及受保护的属性。PEP 8 提倡<strong>对于非公有方法和属性使用单个下划线开头</strong>，只有在避免子类命名冲突时才采用双下划线开头，这是因为解释器会改写双下划线开头的属性，改写为类名 + 变量名的格式。比如下面代码中的 <code>__v3</code> 就被改写为 <code>_C__v3</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span></span><br><span class="line"><span class="meta">... </span>    v1 = <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    _v2 = <span class="number">2</span></span><br><span class="line"><span class="meta">... </span>    __v3 = <span class="number">3</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[_ <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">dir</span>(C) <span class="keyword">if</span> <span class="string">&#x27;v&#x27;</span> <span class="keyword">in</span> _]</span><br><span class="line">[<span class="string">&#x27;_C__v3&#x27;</span>, <span class="string">&#x27;_v2&#x27;</span>, <span class="string">&#x27;v1&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>即便如此，Python 也不能严格保证私有属性不能被外部访问。子类之所以不能访问父类的双下划线开头的属性，只是因为改写后的属性名称不相符而已。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">C</span>):</span> ...</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B._v2</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>B._C__v3</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>对于私有属性，《Effective Python》也建议尽量少用双下划线开头的属性，宁可让子类更多地访问父类的单下划线开头的私有属性，也不要使用双下划线命名限制子类访问，并在文档中把这些属性的合理用法告知子类的开发者。</p>
<p>为什么 Python 不从语法上严格保证私有属性的私密性呢？因为 Python 社区认为开放要比封闭好。而且，Python 提供了一些操作属性的特殊方法，如 <code>__getattr__</code>，使得无法隔绝私有属性的访问，既然如此，那么就默认开发者遵循 Python 编码风格和规范，能够按需操作类内部的属性。</p>
<h2 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h2><p>抽象是解决问题的法宝。良好的抽象策略可以简化问题的复杂度，并且提高系统的通用性和可扩展性。在面向对象程序设计出现直接，面向过程的程序设计多是针对的过程抽象。所谓过程抽象是将问题域中具有明确功能定义的操作抽取出来，将其定义为函数。而面向对象程序设计针对的是数据抽象，是较过程抽象更高级别的抽象方式，通过将描述客体的属性和行为绑定在一起，实现统一的抽象，从而达到对现实世界客体的真正模拟。</p>
<p>类是具有相同属性（数据元素）和行为（功能）的对象的抽象。因此，对象的抽象是类，类的具体化就是对象，也可以说<strong>类是抽象数据类型，对象是类的实例</strong>。类具有属性，它是对象的状态的抽象，用数据结构来存储类的属性。类具有操作，它是对象的行为的抽象，用操作名和实现该操作的方法来描述。类的每一个实例对象都具有这些数据和操作方法。</p>
<p>抽象可以具有层次性，由类的继承结构所体现。高层次的抽象封装行为，为低层次的抽象提供签名，可以不实现具体细节，比如抽象基类或接口。低层次的抽象实现具体细节，提供对象实例化功能。通过抽象的层次性和结构性，可以减小问题求解的复杂度。</p>
<p>从 C++ 2.0 起（1989 年发布），这门语言开始使用抽象类指定接口。Java 的设计者选择不支持类的多重继承，这排除了使用抽象类作为接口规范的可能性，因为一个类通常会实现多个接口。但是，Java 设计者提供了 interface 这个语言结构，以更明确的方式定义接口，并允许一个类实现多个接口 —— 这是一种多重继承。自 Java 8 起，接口可以提供方法实现，即默认方法，使得 Java 中的接口与 C++ 和 Python 中的抽象基类更像了。但它们之间有个关键的区别：Java 的接口没有状态。Java 之后使用最广泛的 JVM 语言要数 Scala 了，它就实现了性状（trait）。不管怎么说，让我们先从抽象基类开始，了解 Python 中的高层次抽象类型。</p>
<h3 id="抽象基类"><a href="#抽象基类" class="headerlink" title="抽象基类"></a>抽象基类</h3><p>在引入抽象基类（Abstract base classes，缩写 ABC）之前，Python 就已经很成功了。Python 倡导使用<strong>鸭子类型和协议</strong>，忽略对象的真正类型，转而关注对象有没有实现所需的方法、签名和语义。这使得 Python 编码更加宽松，不需要严格的类型限制。因此抽象基类并不是 Python 的第一选择，大概也因为此，直至 Python 语言诞生 15 年后，Python 2.6 中才引入抽象基类。</p>
<p>但这并不意味着抽象基类一无是处，相反，它被广泛应用于 Java、C# 等面向对象语言中。抽象基类的常见用途是实现接口时作为基类使用，它与普通基类的区别在于：</p>
<ul>
<li>抽象基类不能实例化；</li>
<li>具体子类必须实现抽象基类的抽象方法。</li>
</ul>
<p>正是由于抽象基类限定了子类必须实现特定的方法，它被经常用于构建框架。你可以在 Python 标准库的 <code>collections.abc</code> 和 <code>numbers</code> 模块中见到抽象基类的身影。</p>
<p>Python 中定义抽象基类需要用到标准库提供的 <code>abc</code> 模块，该模块由 <a target="_blank" rel="noopener" href="https://www.python.org/dev/peps/pep-3119/">PEP 3119 – Introducing Abstract Base Classes</a> 提案所引入。它支持两种方式定义抽象基类，一种是使用 <code>abc.ABCMeta</code> 作为元类。声明元类的 <code>metaclass</code> 关键字参数是 Python 3 引入的，在此之前 Python 2 还得使用 <code>__metaclass__</code> 类属性。另一种是直接继承 <code>abc.ABC</code> 类，需要注意 ABC 的类型仍然是 ABCMeta。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, ABCMeta</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">type</span>(ABC)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">abc</span>.<span class="title">ABCMeta</span>&#x27;&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">class</span> <span class="title">MyABC</span>(<span class="params">metaclass=ABCMeta</span>):</span> </span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyABC</span>(<span class="params">ABC</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line"><span class="meta">... </span></span><br></pre></td></tr></table></figure>

<p>一般通过继承 <code>ABC</code> 来简单地创建抽象基类，当遇到可能会导致元类冲突的多重继承时，也可以使用 <code>ABCMeta</code> 作为元类来定义抽象基类。</p>
<p><code>abc</code> 模块还提供了用于声明<strong>抽象方法</strong>的装饰器 <code>@abstractmethod</code>。抽象方法定义体中通常只有文档字符串。在导入时，Python 不会检查抽象方法是否被实现，而是在实例化时检查。如果没有实现，将抛出 <code>TypeError</code> 异常提示无法实例化抽象类。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> abc <span class="keyword">import</span> abstractmethod</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyABC</span>(<span class="params">ABC</span>):</span></span><br><span class="line"><span class="meta">... </span>    @abstractmethod</span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">my_abstract_method</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="string">&quot;&quot;&quot;My abstract method&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">C</span>(<span class="params">MyABC</span>):</span> </span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>C()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: Can<span class="string">&#x27;t instantiate abstract class C with abstract methods my_abstract_method</span></span><br></pre></td></tr></table></figure>

<p>抽象方法可以有实现代码，但即便实现了，子类也必须覆盖抽象方法。通常这样做的目的是在子类中使用 <code>super()</code> 复用基类的方法，为它添加功能而不是从头实现。其实在抽象基类出现之前，抽象方法会抛出 <code>NotImplementedError</code> 异常，提示子类必须实现该抽象方法。</p>
<p>除了 <code>@abstractmethod</code> 之外，<code>abc</code> 模块还定义了 <code>@abstractclassmethod</code>、<code>@abstractstaticmethod</code> 和 <code>@abstractproperty</code> 装饰器，可用于装饰类方法、静态方法和特性。但自 Python 3.3 起这三个装饰器就被废弃了，因为可以使用装饰器堆叠达到同样的效果。在堆叠时，要保证 <code>@abstractmethod</code> 是最内层的装饰器，即最靠近函数定义体。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyABC</span>(<span class="params">ABC</span>):</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_abstract_classmethod</span>(<span class="params">cls, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_abstract_staticmethod</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_abstract_property</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line"><span class="meta">    @my_abstract_property.setter</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">my_abstract_property</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><strong>注</strong>：PEP 3119 提案同时也引入并定义了集合类型的抽象基类，包括容器和迭代器类型，可以参考提案的 <a target="_blank" rel="noopener" href="https://www.python.org/dev/peps/pep-3119/#abcs-for-containers-and-iterators">ABCs for Containers and Iterators</a> 一节。这些集合类型被统一定义在 <code>collections.abc</code> 模块中。为了减少加载时间，Python 3.4 在 <code>collections</code> 包之外实现了这个模块，即 <code>_collections_abc.py</code>，所以在导入时要使用 <code>collections.abc</code> 与 <code>collections</code> 分开导入。</p>
<h3 id="注册虚拟子类"><a href="#注册虚拟子类" class="headerlink" title="注册虚拟子类"></a>注册虚拟子类</h3><p>Python 中的抽象基类还有一个重要的实用优势：可以使用 register 将某个类声明为一个抽象基类的“虚拟”子类，这样就不用显式继承。这打破了继承的强耦合，与面向对象编程的惯有知识有很大出入，因此在使用继承时要多加小心。</p>
<p>虚拟子类不会从抽象基类中继承任何方法和属性，但类型检查函数 <code>issubclass()</code> 和 <code>isinstance()</code> 都会通过。Python 不会检查虚拟子类是否符合抽象基类的接口，即便实例化时也不会检查，但会在调用时抛出异常。因此，为了避免运行时错误，虚拟子类要实现抽象基类的所有抽象方法。</p>
<p>注册虚拟子类的 <code>register()</code> 方法可以作为普通函数调用，也可以作为装饰器使用。如下定义的抽象基类 Drawable 中定义了一个抽象方法 draw，可以随机抽取一个元素。我们实现了一个扑克类 Poker，使用装饰器形式将 Poker 类注册为 Drawable 的虚拟子类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Drawable</span>(<span class="params">ABC</span>):</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Random draw an item.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Drawable.register</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Poker</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.suits = (<span class="string">&#x27;Spade&#x27;</span>, <span class="string">&#x27;Heart&#x27;</span>, <span class="string">&#x27;Diamond&#x27;</span>, <span class="string">&#x27;Club&#x27;</span>)</span><br><span class="line">        self.numbers = (*<span class="built_in">range</span>(<span class="number">2</span>, <span class="number">11</span>), <span class="string">&#x27;J&#x27;</span>, <span class="string">&#x27;Q&#x27;</span>, <span class="string">&#x27;K&#x27;</span>, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">52</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">self</span>):</span></span><br><span class="line">        suit = self.suits[random.randint(<span class="number">0</span>, <span class="number">3</span>)]</span><br><span class="line">        number = self.numbers[random.randint(<span class="number">0</span>, <span class="number">12</span>)]</span><br><span class="line">        <span class="keyword">return</span> suit, number</span><br></pre></td></tr></table></figure>

<p>使用 <code>@Drawable.register</code> 与直接调用方法的 <code>Drawable.register(Poker)</code> 效果相同，这样即使不用显式继承，Poker 类也会被解释器视为 Drawable 抽象基类的子类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>poker = Poker()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>poker.draw()</span><br><span class="line">(<span class="string">&#x27;Diamond&#x27;</span>, <span class="string">&#x27;Q&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">issubclass</span>(Poker, Drawable)</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">isinstance</span>(poker, Drawable)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>注册虚拟子类被广泛应用于 <code>collections.abc</code> 模块中，比如将内置类型 tuple、str、range 和 memoryview 注册为序列类型 Sequence 的虚拟子类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Sequence</span>.register(<span class="built_in">tuple</span>)</span><br><span class="line"><span class="type">Sequence</span>.register(<span class="built_in">str</span>)</span><br><span class="line"><span class="type">Sequence</span>.register(<span class="built_in">range</span>)</span><br><span class="line"><span class="type">Sequence</span>.register(<span class="built_in">memoryview</span>)</span><br></pre></td></tr></table></figure>

<h4 id="subclasshook-钩子方法"><a href="#subclasshook-钩子方法" class="headerlink" title="__subclasshook__ 钩子方法"></a><code>__subclasshook__</code> 钩子方法</h4><p>有时甚至不需要手动注册，抽象基类也能将一个类绑定为虚拟子类。比如 Poker 类会被绑定为 <code>collections.abc.Sized</code> 的虚拟子类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections.abc <span class="keyword">import</span> Sized</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">issubclass</span>(Poker, Sized)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p>这是由于 Sized 抽象基类内部实现了一个名为 <code>__subclasshook__</code> 的钩子方法。这个方法会去检查类中是否包含 <code>__len__</code> 方法，如果包含，那么类型检查 <code>issubclass()</code> 和 <code>isinstance()</code> 会返回 True。</p>
<p>同理，我们可以为 Drawable 类实现此钩子方法，方法的两个参数指代的均是类对象 <code>__class__</code>。这样即使不手动注册，实现了 draw 方法的扭蛋机类 Capsule 也会被判定为 Drawable 的子类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Drawable</span>(<span class="params">ABC</span>):</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Random draw an item.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__subclasshook__</span>(<span class="params">cls, C</span>):</span></span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> Drawable:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;draw&#x27;</span> <span class="keyword">in</span> <span class="built_in">dir</span>(C)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NotImplemented</span></span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Capsule</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">draw</span>(<span class="params">self</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="comment"># &gt;&gt;&gt; issubclass(Capsule, Drawable)</span></span><br><span class="line"><span class="comment"># True</span></span><br></pre></td></tr></table></figure>

<p>实际上，很少需要为自己编写的抽象基类实现 <code>__subclasshook__</code> 方法，虽然这符合 Python 对于“鸭子类型”的定义，但这样做可靠性很低。好比说，不能指望任何实现了 draw 方法的类都是 Drawable 类型（可随机抽取元素），因为 draw 可能还指代其他语义，比如画图。</p>
<p>类型检查函数 <code>issubclass()</code> 和 <code>isinstance()</code> 之所以会返回 True，是由于定义在 ABCMeta 类中的 <code>__subclasscheck__</code>  和 <code>__instancecheck__</code> 特殊方法会覆盖其行为。在 ABCMeta 的构造方法中定义了一些 WeakSet 类型的类属性：<code>_abc_registry</code>、<code>_abc_cache</code> 和 <code>_abc_negative_cache</code>，它们会动态的存放抽象基类的虚拟子类（类型检查时会动态添加）。可以通过调试接口 <code>_dump_registry()</code> 查看一个抽象基类的虚拟子类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>Drawable._dump_registry()</span><br><span class="line">Class: __main__.Drawable</span><br><span class="line">Inv. counter: <span class="number">45</span></span><br><span class="line">_abc_registry: <span class="built_in">set</span>()</span><br><span class="line">_abc_cache: &#123;&lt;weakref at <span class="number">0x10b0e5220</span>; to <span class="string">&#x27;type&#x27;</span> at <span class="number">0x7f9c88f3e380</span> (Poker)&gt;, &lt;weakref at <span class="number">0x10b0fe9a0</span>; to <span class="string">&#x27;type&#x27;</span> at <span class="number">0x7f9c88cb0dc0</span> (Capsule)&gt;&#125;</span><br><span class="line">_abc_negative_cache: <span class="built_in">set</span>()</span><br><span class="line">_abc_negative_cache_version: <span class="number">45</span></span><br></pre></td></tr></table></figure>

<p><strong>注</strong>：ABCMeta 类的 Python 源码可以在 <code>_py_abc</code> 模块中查看。CPython 提供了一套基于 C 语言的更高效实现，仅在其导入失败时，才导入 <code>_py_abc</code> 模块中的 ABCMeta。</p>
<h4 id="白鹅类型"><a href="#白鹅类型" class="headerlink" title="白鹅类型"></a>白鹅类型</h4><p>最早提出“鸭子类型”的 Alex Martelli 建议在鸭子类型的基础上添加“白鹅类型”。白鹅类型是指，只要 cls 是抽象基类，即 cls 的元类是 <code>abc.ABCMeta</code>，就可以使用 <code>isinstance(obj, cls)</code>。事实上，虚拟子类就是一种白鹅类型，当我们向抽象基类注册一个虚拟子类时，ABCMeta 会将该类保存在抽象基类的类属性中，以供类型检查使用。</p>
<p>虚拟子类是抽象基类动态性的体现，也是符合 Python 风格的方式。它允许我们动态地改变类的属别关系。抽象基类定义了一系列方法，并给出了方法应当实现的功能，在这一层次上，“白鹅类型”能够对类型进行甄别。当一个类继承自抽象基类时，语言本身限制了该类必须完成抽象基类定义的语义；当一个类注册为虚拟子类时，限制则来自于编写者自身（成年人）。两种类都能通过“白鹅类型”的校验，不过虚拟子类提供了更好的灵活性与扩展性。例如，一个框架允许第三方插件时，采用虚拟子类即可以明晰接口，又不会影响内部的实现。</p>
<h3 id="自定义的抽象基类"><a href="#自定义的抽象基类" class="headerlink" title="自定义的抽象基类"></a>自定义的抽象基类</h3><p>在各类编程语言中，基础的数据结构都必不可少，比如链表、堆栈、集合等，对于这些数据结构，语言会对它们进行抽象，定义接口并设计一套继承体系。Java 将这些数据结构统称为容器（意指用于容纳数据），从最底层的 List、Set、Map 等接口，到 AbstractList、AbstractSet、AbstractMap 等抽象类，再到最上层的 ArrayList、HashSet、HashMap 等具体实现类，越靠近上层的类方法越丰富，但底层的接口和抽象类是框架的骨架，构成了整个容器框架。同时，接口和抽象类也是体现 Java 语言多态性（向上转型）的重要设计。</p>
<p>Python 的容器抽象基类定义在 <code>collections.abc</code> 模块中，其 UML 类图如下所示：</p>
<p><img data-src="/2021/05/12/Python-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/uml.png" alt="162525d70561a8d42f3e5007ef176c0c.png"></p>
<p>下面我们尝试使用 Python 中的抽象基类实现一个自定义的容器类型。为了简化，这个容器类型仅支持新增 <code>push()</code> 和删除 <code>pop()</code> 两个操作。由于不同子类的具体实现不同，比如栈是先进后出，队列是先进先出，所以这两个方法被定义为抽象方法。此外还提供了检视容器元素的 <code>inspect()</code> 方法，由于实现逻辑相同，因此 <code>inspect()</code> 可以是具体方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> abc <span class="keyword">import</span> ABC, abstractmethod</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Container</span>(<span class="params">ABC</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, iterable=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> iterable:</span><br><span class="line">            self._items = <span class="built_in">list</span>(iterable)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._items = []</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Remove and return item. Raises IndexError if container is empty.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Add item.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inspect</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._items</span><br></pre></td></tr></table></figure>

<p>可以看到，抽象基类既可以拥有抽象方法，也可以拥有具体方法。代码中的初始化方法 <code>__init__</code> 和 <code>inspect()</code> 方法都是具体方法，但子类依然可以覆盖具体方法，或者使用 <code>super()</code> 调用它们进行功能增强。</p>
<h3 id="自定义的抽象基类的子类"><a href="#自定义的抽象基类的子类" class="headerlink" title="自定义的抽象基类的子类"></a>自定义的抽象基类的子类</h3><p>接下来我们实现 Container 抽象基类的两个具体子类：栈和队列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>(<span class="params">Container</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">len</span>(self._items):</span><br><span class="line">            <span class="keyword">raise</span> IndexError(<span class="string">&#x27;Stack is empty.&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self._items.pop()  <span class="comment"># remove last item</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        self._items.append(item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Queue</span>(<span class="params">Container</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">len</span>(self._items):</span><br><span class="line">            <span class="keyword">raise</span> IndexError(<span class="string">&#x27;Queue is empty.&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self._items.pop(<span class="number">0</span>)  <span class="comment"># remove first item</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        self._items.append(item)</span><br></pre></td></tr></table></figure>

<p>由于具体子类继承自抽象基类 Container，所以在类声明时必须明确指定类扩展自 Container 类。并且覆盖抽象基类中的两个抽象方法。栈和队列的 <code>pop()</code> 行为不同，栈满足先进后出，而队列满足先进先出，并在容器为空时均抛出 IndexError 异常。接下来验证栈和队列的特性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack = Stack([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack.push(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack.pop()</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>stack.inspect()</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>queue = Queue([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>queue.push(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>queue.pop()</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>queue.inspect()</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h3 id="何时使用抽象基类"><a href="#何时使用抽象基类" class="headerlink" title="何时使用抽象基类"></a>何时使用抽象基类</h3><p>对于简单的个人应用，优先使用现有的抽象基类，而不是自己编写抽象基类，因为这很容易造成过度设计。毕竟对于 Python 来说，“简单”永远是这门语言的核心，滥用抽象基类会造成灾难性后果，太注重语言的表面形式对于以实用和务实著称的 Python 可不是好事。</p>
<p>抽象基类可以约束各个子类实现相同的一套 API。除此之外，抽象基类的一个用途是运行时的类型检查，可以使用 <code>isinstance()</code> 检查某个对象是否是抽象基类的子类型，即是否实现了特定的接口。这便于我们对于不同的情形进行分支处理或异常捕获。</p>
<p>尽管抽象基类使得类型检查变得更容易了，但也不该过度使用它。Python 的核心在于它是一门动态语言，如果处处都强制实现类型约束，那么会使代码变得复杂且丑陋。我们应该拥抱 Python 的灵活性。</p>
<p>因此对于抽象基类的使用，要着重声明：除非构建的是允许用户扩展的框架，否则不要轻易定义抽象基类。日常使用中，我们与抽象基类的联系应该是创建现有抽象基类的子类。当你拥有像创建新的抽象基类的想法时，首先尝试使用常规的鸭子类型来解决问题。</p>
<h2 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h2><p>面向对象编程语言的一个重要功能就是“继承”，它可以使得在现有类的基础上，无需编写重复代码就可以实现功能的扩展。继承体现了从一般到特殊的过程。</p>
<p>通过继承创建的新类称为“子类”或“派生类”，被继承的类称为“基类”、“父类”或“超类”。在某些面向对象语言中，一个子类可以继承自多个父类，这称为多重继承。Python 是一门支持多重继承的语言。</p>
<p>Python 的继承句法是，在类声明的括号中添加父类名，如 <code>class C(Base):</code> 声明了类 C 继承自基类 Base。当声明多重继承时，使用逗号隔开，如 <code>class C(BaseA，BaseB):</code>。</p>
<p>子类会继承父类的非私有属性和方法，包括类属性。这里的私有属性是指以双下划线开头且不以双下划线结尾命名的属性，由于 Python 的名称改写机制，这类私有属性将会被改写为“类名 + 属性名”的格式，所以不能被子类通过原有名称访问。</p>
<p>如下，B 类派生自 A 类，继承了 A 类的所有非私有属性和方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line"><span class="meta">... </span>    attr1 = <span class="number">1</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>        self.attr2 = <span class="number">2</span> </span><br><span class="line"><span class="meta">... </span>        self._attr3 = <span class="number">3</span></span><br><span class="line"><span class="meta">... </span>        self.__attr4 = <span class="number">4</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">method</span>(<span class="params">self</span>):</span> </span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&#x27;class A method&#x27;</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = B()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.attr1</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.attr2</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b._attr3</span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.__attr4</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">&#x27;B&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;__attr4&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.method()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="title">method</span></span></span><br></pre></td></tr></table></figure>

<p>子类可以覆盖父类的属性和方法，或者使用 <code>super()</code> 调用父类方法，在原有方法基础上添加新功能。<code>super()</code> 的一个重要用途是用于初始化方法 <code>__init__</code> 中。如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line"><span class="meta">... </span>    attr1 = <span class="string">&#x27;b1&#x27;</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>().__init__()</span><br><span class="line"><span class="meta">... </span>        self.attr5 = <span class="number">5</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">method</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>().method()</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&#x27;class B method&#x27;</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = B()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.attr1</span><br><span class="line"><span class="string">&#x27;b1&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.__dict__</span><br><span class="line">&#123;<span class="string">&#x27;attr2&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;_attr3&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;_A__attr4&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;attr5&#x27;</span>: <span class="number">5</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.method()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> <span class="title">method</span></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">B</span> <span class="title">method</span></span></span><br></pre></td></tr></table></figure>

<h3 id="多重继承"><a href="#多重继承" class="headerlink" title="多重继承"></a>多重继承</h3><p>Python 支持多重继承。人们对于多重继承褒贬不一，C++ 中对于多重继承的滥用一直饱受诟病，借鉴自 C++ 的 Java 选择直接移除了多重继承特性，采用接口（Interface）作为代替，并取得了巨大的成功。事实证明，接口是一种更加优雅的多重继承解决方案。</p>
<p>多重继承首先要解决的问题就是潜在的<strong>命名冲突</strong>，如果一个类继承自两个不相关的类，这两个类拥有实现不同的同名方法，那么该调用哪一个？这种冲突被称为“菱形问题”。为了解决这个问题，Python 会按照特定的顺序遍历继承图。这个顺序称为<strong>方法解析顺序</strong>（Method Resolution Order，缩写 MRO）。<strong>类有一个名为 <code>__mro__</code> 的类属性，它的值是一个元组，按照方法解析顺序存放各个超类的名称</strong>。</p>
<h4 id="mro-方法解析顺序"><a href="#mro-方法解析顺序" class="headerlink" title="__mro__ 方法解析顺序"></a><code>__mro__</code> 方法解析顺序</h4><p>我们定义一个继承结构，类 D 继承自类 B 和 C，而类 B 和 C 又都继承自类 A。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class A:&#x27;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class B:&#x27;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class C:&#x27;</span>, self)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span>(<span class="params">B, C</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>从继承结构上来看，这是一个菱形结构，会存在调用同名方法的二义性。那么，调用 D 实例的 <code>speak()</code> 方法会去调用哪个父类呢？</p>
<p>答案是会调用 B 的 <code>speak()</code> 方法。D 类的 <code>__mro__</code> 属性如下，访问 D 的方法，会按照 <code>D -&gt; B -&gt; C -&gt; A</code> 的顺序进行解析。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = D()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d.speak()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>:</span> &lt;multiple_inheritance.D <span class="built_in">object</span> at <span class="number">0x1086a6580</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D.__mro__</span><br><span class="line">(&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">multiple_inheritance</span>.<span class="title">D</span>&#x27;&gt;, &lt;<span class="title">class</span> &#x27;<span class="title">multiple_inheritance</span>.<span class="title">B</span>&#x27;&gt;, &lt;<span class="title">class</span> &#x27;<span class="title">multiple_inheritance</span>.<span class="title">C</span>&#x27;&gt;, &lt;<span class="title">class</span> &#x27;<span class="title">multiple_inheritance</span>.<span class="title">A</span>&#x27;&gt;, &lt;<span class="title">class</span> &#x27;<span class="title">object</span>&#x27;&gt;)</span></span><br></pre></td></tr></table></figure>

<p><strong>注</strong>：方法解析顺序不会列出虚拟子类的被注册超类。因此虚拟子类也不会从被注册超类中继承任何方法。</p>
<h4 id="super-调用链"><a href="#super-调用链" class="headerlink" title="super() 调用链"></a><code>super()</code> 调用链</h4><p>在使用 <code>super()</code> 调用父类方法时，也遵循方法解析顺序。如果父类中的方法也包含 <code>super()</code> 语句，则按照方法解析顺序调用下一个父类的方法（下一个父类可能不是当前父类的直接父类）。比如如下添加了 <code>super()</code> 语句的 <code>speak()</code> 方法打印如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class A:&#x27;</span>, self)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().speak()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class B:&#x27;</span>, self)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().speak()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;class C:&#x27;</span>, self)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span>(<span class="params">B, C</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">speak</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().speak()</span><br><span class="line">        </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>D().speak()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span> &lt;__main__.D <span class="built_in">object</span> at <span class="number">0x10edc8b80</span>&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span>:</span> &lt;__main__.D <span class="built_in">object</span> at <span class="number">0x10edc8b80</span>&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>:</span> &lt;__main__.D <span class="built_in">object</span> at <span class="number">0x10edc8b80</span>&gt;</span><br></pre></td></tr></table></figure>

<p>按照 <code>D -&gt; B -&gt; C -&gt; A</code> 的方法解析顺序，D 中的 <code>super()</code> 方法跳转到 B，B 中的 <code>super()</code> 方法跳转到 C（而不是 B 的直接父类 A），C 中的 <code>super()</code> 方法再跳转到 A。由于 <code>super()</code> 语句在 print 语句之前，最终呈现出的打印顺序是方法解析顺序的出栈顺序。</p>
<h4 id="方法解析顺序的单调性"><a href="#方法解析顺序的单调性" class="headerlink" title="方法解析顺序的单调性"></a>方法解析顺序的单调性</h4><p>方法解析顺序不仅考虑继承图，还考虑子类声明中所列的超类顺序。如果 D 类声明为 <code>class D(B, C):</code>，那么 D 类一定会先于 B、C 类被搜索，且 B 类一定先于 C 类被搜索。我们将这种 <code>D -&gt; B -&gt; C</code> 的顺序称为<strong>方法解析顺序的单调性</strong>。 用户在定义继承关系时必须要遵循单调性原则。</p>
<p>Python 方法解析顺序采用的 C3 算法会检查方法解析顺序的单调性。简单地说，C3 算法的基本逻辑是，每定义好一个继承关系顺序，算法会将所有顺序按照满足单调性的方式整合起来，如果整合过程出现冲突，算法会抛出错误。</p>
<p>如下所示，由于定义 B 类时声明为 <code>class B(A):</code>，所以 B 的解析顺序要先于 A，然而在使用 <code>class C(A, B):</code> 声明 C 类时，A 的解析顺序又先于 B，因此发生冲突，抛出异常。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span> ...</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span> ...</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">C</span>(<span class="params">A, B</span>):</span> ...</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">TypeError: Cannot create a consistent method resolution</span><br><span class="line">order (MRO) <span class="keyword">for</span> bases A, B</span><br></pre></td></tr></table></figure>

<p>在 Python 标准库中，最常使用多重继承的是 <code>collections.abc</code> 模块，其中的类都定义为抽象基类。抽象基类类似于 Java 中的接口声明，只不过它可以提供具体方法。因此在 <code>collections.abc</code> 模块中频繁使用多重继承并没有问题，它为 Python 的集合类型构建了一个继承体系。然而，滥用多重继承容易得到令人费解和脆弱的设计。《Effective Python》中也提到：只在使用混入时才使用多重继承。为此，有必要先介绍一下混入类。</p>
<h3 id="混入类"><a href="#混入类" class="headerlink" title="混入类"></a>混入类</h3><p>除了传统的面向对象继承方式，还流行一种通过可重用组件创建类的方式，那就是<strong>混入</strong>（mixin），这在 Scala 和 JavaScript 使用颇多。如果一个类的作用是为多个不相关的子类提供方法实现，从而实现重用，但不体现 “is-a” 语义，应该把这个类明确定义为混入类。从概念上讲，混入不定义新类型，只是打包方法，便于重用。因此，<strong>混入类绝对不能实例化，而且具体类不能只继承混入类</strong>。</p>
<p>Python 没有提供定义混入类的专有关键字，而是推荐在名称末尾加上 “Mixin” 后缀。而在 Scala 中，使用 trait（特性）关键字来声明混入类，TypeScript 中则使用 implements 关键字来继承混入类。</p>
<p>抽象基类可以实现具体方法，因此也可以作为混入使用。<code>collections.abc</code> 模块中的抽象基类在一定程度上可以被视为混入类，它们都声明了 <code>__slots__ = ()</code> 语句，表明了混入类不能具有实例属性，即混入类不能被实例化。但是，抽象基类可以定义某个抽象类型，而混入做不到，因此，抽象基类可以作为其他类的唯一基类，而混入绝不能作为唯一超类。但是，抽象基类有个局限是混入类没有的，即：抽象基类中提供具体实现的抽象方法只能与抽象基类及其超类中的方法协作。</p>
<p>一些三方库和框架中也有用到混入，比如 Django 框架，我截取了 <a target="_blank" rel="noopener" href="https://github.com/django/django/blob/main/django/views/generic/base.py">Django 视图模块</a>的一小部分源码，以便更好的理解混入类与多重继承的关系。</p>
<h4 id="Django-源码"><a href="#Django-源码" class="headerlink" title="Django 源码"></a>Django 源码</h4><p>在 Django 中，视图是可调用对象，它的参数是表示 HTTP 请求的对象，返回值是一个表示 HTTP 响应的对象。我们要关注的是这些响应对象。响应可以是简单的重定向，没有主体内容，为我们导向另一个 url，也可以是复杂的网页内容，需要使用 HTML 模版渲染，最终呈现在浏览器终端上。为此，Django 框架提供了重定向视图 RedirectView，以及模版视图 TemplateView。</p>
<p>我们将注意力放在 TemplateView 类上，它继承自三个类，从左到右分别是模版响应混入类 TemplateResponseMixin、上下文混入类 ContextMixin，以及视图基类 View。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemplateView</span>(<span class="params">TemplateResponseMixin, ContextMixin, View</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Render a template. Pass keyword arguments from the URLconf to the context.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, request, *args, **kwargs</span>):</span> </span><br><span class="line">        context = self.get_context_data(**kwargs)</span><br><span class="line">        <span class="keyword">return</span> self.render_to_response(context)</span><br></pre></td></tr></table></figure>

<p>从类型上来说，TemplateView 依然是一个视图类型。View 是所有视图的基类，提供核心功能，如 dispatch 方法。RedirectView 由于不需要渲染，所以只继承了 View 类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">View</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Intentionally simple parent class for all views. Only implements</span></span><br><span class="line"><span class="string">    dispatch-by-method and simple sanity checking.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    http_method_names = [<span class="string">&#x27;get&#x27;</span>, <span class="string">&#x27;post&#x27;</span>, <span class="string">&#x27;put&#x27;</span>, <span class="string">&#x27;patch&#x27;</span>, <span class="string">&#x27;delete&#x27;</span>, <span class="string">&#x27;head&#x27;</span>, <span class="string">&#x27;options&#x27;</span>, <span class="string">&#x27;trace&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">as_view</span>(<span class="params">cls, **initkwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup</span>(<span class="params">self, request, *args, **kwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dispatch</span>(<span class="params">self, request, *args, **kwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">http_method_not_allowed</span>(<span class="params">self, request, *args, **kwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">options</span>(<span class="params">self, request, *args, **kwargs</span>):</span> ...</span><br></pre></td></tr></table></figure>

<p>两个混入类 TemplateResponseMixin 和 ContextMixin 并不代表某一特定类型，而是打包了若干属性和方法，此类方法又不是 RedirectView 所需要的，因此不能定义在 View 基类中。TemplateResponseMixin 提供的功能只针对需要使用模版的视图，除了 TemplateView 还提供给其他视图，例如用于渲染列表的 ListView 视图以及默认视图 DetailView 等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemplateResponseMixin</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;A mixin that can be used to render a template.&quot;&quot;&quot;</span></span><br><span class="line">    template_name = <span class="literal">None</span></span><br><span class="line">    template_engine = <span class="literal">None</span></span><br><span class="line">    response_class = TemplateResponse</span><br><span class="line">    content_type = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render_to_response</span>(<span class="params">self, context, **response_kwargs</span>):</span> ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_template_names</span>(<span class="params">self</span>):</span> ...</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContextMixin</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A default context mixin that passes the keyword arguments received by</span></span><br><span class="line"><span class="string">    get_context_data() as the template context.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    extra_context = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_context_data</span>(<span class="params">self, **kwargs</span>):</span> ...</span><br></pre></td></tr></table></figure>

<p>Django 基于类的视图 API 是多重继承的一个优雅示例，尤其是 Django 的混入类易于理解：各个混入类的目的明确，且都以 “Mixin” 作为后缀。</p>
<h3 id="继承的最佳实践"><a href="#继承的最佳实践" class="headerlink" title="继承的最佳实践"></a>继承的最佳实践</h3><p><strong>明确使用继承的目的</strong>：在决定使用继承之前，首先明确这么做的目的。如果是为了继承重用代码，那么组合和委托也可以达到相同效果。《设计模式：可复用面向对象软件的基础》一书中明确指出：“<strong>优先使用对象组合，而不是类继承</strong>”。组合体现的是 “has-a” 语义，与继承相比，组合的耦合性更低，可扩展性更高。继承并不是银弹，继承意味着父类与子类的强耦合性，一旦父类接口发生变化，所有子类都会受到影响。如果继承用错了场合，那么后期的维护可能是灾难性的。但如果目的是继承接口，创建子类型，实现 “is-a” 关系，那么使用继承是合适的。<strong>接口继承是框架的支柱</strong>，如果类的作用是定义接口，就应该明确定义为抽象基类，就像 <code>collections.abc</code> 模块所做的那样。</p>
<p><strong>不要继承多个具体类</strong>：最多只有一个具体父类，也可以没有。也就是说，除了这一个具体父类之外，其余都是抽象基类或混入。并且，如果抽象基类或混入的组合被经常使用，那么就可以考虑定义一个聚合类，使用易于理解的方式将他们结合起来，就如同 <code>collections.abc</code> 模块中定义的 Collection 类：<code>class Collection(Sized, Iterable, Container):</code>。</p>
<p><strong>只在使用混入时才使用多重继承</strong>：这比上一条要更加严苛，尽管抽象基类有时可被视为混入类。不管怎么说，如果不是开发框架，尽量避免使用多重继承，如果不得不用多重继承，请使用混入类。混入类不会破坏现有的继承结构树，它就像小型的可插拔的扩展接口坞，目的不是声明 “is-a” 关系，而是为子类扩展特定功能。所以有时也将混入类称为混入组件。</p>
<p><strong>在声明多重继承自混入类和基类时，先声明混入类，最后声明基类</strong>：这是由于，在定义混入类时使用 <code>super()</code> 是普遍的。为了保证继承自混入类和基类的子类，在调用方法时会执行基类的同名方法，需要先声明混入类再声明基类。这样，按照方法解析顺序的单调性，混入类中的 <code>super()</code> 方法会调用到基类中的方法。</p>
<p>如下定义了一个属性只能赋值一次的字典，为其属性赋值时，按照方法解析顺序，会先调用混入类的 <code>__setitem__</code> 方法，执行到 <code>super()</code> 语句，调用基类 UserDict 的 <code>__setitem__</code> 方法进行设值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> UserDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SetOnceMappingMixin</span>:</span></span><br><span class="line">    __slots__ = ()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="built_in">str</span>(key) + <span class="string">&#x27; already set&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().__setitem__(key, value)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SetOnceDefaultDict</span>(<span class="params">SetOnceMappingMixin, UserDict</span>):</span> ...</span><br></pre></td></tr></table></figure>

<p><strong>使用 <code>collections</code> 模块子类化内置类型</strong>：内置类型的原生方法使用 C 语言实现，不会调用子类中覆盖的方法。比如，如下 DoubleDict 中定义的 <code>__setitem__</code> 方法并不会覆盖初始化方法 <code>__init__</code> 中的设值方法。因此，需要定制 list、dict 或 str 类型时，应该使用 <code>collections</code> 模块中的 UserList、UserDict 或 UserString 等。这些类是对内置类型的包装，会把操作委托给内置类型 —— 这是标准库中优先选择组合而不是继承的又一例证。如果所需的行为与内置类型区别很大，那么子类化 <code>collections.abc</code> 中的抽象基类自己实现或许更加容易。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">DoubleDict</span>(<span class="params"><span class="built_in">dict</span></span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>().__setitem__(key, value * <span class="number">2</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = DoubleDict(a=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> collections <span class="keyword">import</span> UserDict</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">DoppelDict</span>(<span class="params">UserDict</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span>(<span class="params">self, key, value</span>):</span></span><br><span class="line"><span class="meta">... </span>        <span class="built_in">super</span>().__setitem__(key, value * <span class="number">2</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = DoppelDict(a=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="s1mple"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">s1mple</p>
  <div class="site-description" itemprop="description">春光恰与少年同，十里清风慕天青</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">76</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/s1mplecc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;s1mplecc" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:s1mple951205@gmail.com" title="E-Mail → mailto:s1mple951205@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">s1mple</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 33231,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
